1 lazy
2 s"name=${name}, age=${age}, sex=${sex}"
3 三引号使用
val/var 变量名 = """字符串1
字符串2"""
4 scala中没有，++、--运算符
与Java不一样，在scala中，可以直接使用==、!=进行比较，它们与equals方法表示一致。而比较两个对象的引用值，使用eq
5 类型	说明
Any	所有类型的父类，,它有两个子类AnyRef与AnyVal
AnyVal	所有数值类型的父类
AnyRef	所有对象类型（引用类型）的父类
Unit	表示空，Unit是AnyVal的子类，它只有一个的实例() 
它类似于Java中的void，但scala要比Java更加面向对象
Null	Null是AnyRef的子类，也就是说它是所有引用类型的子类。它的实例是null
可以将null赋值给任何对象类型
Nothing	所有类型的子类
不能直接创建该类型实例，某个方法抛出异常时，返回的就是Nothing类型，因为Nothing是所有类的子类，那么它可以赋值为任何类型

val b:Int = null
scala会解释报错：
Null类型并不能转换为Int类型，说明Null类型并不是Int类型的子类

Range类型
val range: Range.Inclusive = 1 to 10
val array: Array[Int] = range.toArray
val toArray: Array[Int] = (1 to 10).toArray

	
6 块表达式
scala中，使用{}表示一个块表达式
和if表达式一样，块表达式也是有值的
值就是最后一个表达式的值
问题
请问以下代码，变量a的值是什么？
scala> val a = {
     | println("1 + 1")
     | 1 + 1
     | }

守卫
for表达式中，可以添加if判断语句，这个if判断就称之为守卫。我们可以使用守卫让for表达式更简洁。
语法
for(i <- 表达式/数组/集合 if 表达式) {
    // 表达式
}

for推导式
将来可以使用for推导式生成一个新的集合（一组数据）

在for循环体中，可以使用yield表达式构建出一个集合，我们把使用yield的for表达式称之为推导式
示例
生成一个10、20、30...100的集合
参考代码
// for推导式：for表达式中以yield开始，该for表达式会构建出一个集合
val v = for(i <- 1 to 10) yield i * 10

8 实现break是用breakable{}将整个for表达式包起来，而实现continue是用breakable{}将for表达式的循环体包含起来就可以了

9 定义递归方法，不能省略返回值类型

10 在scala中，有以下几种方法调用方式，
后缀调用法
中缀调用法
花括号调用法
无括号调用法
后缀调用法
这种方法与Java没有区别。
语法
对象名.方法名(参数)
示例
使用后缀法Math.abs求绝对值
参考代码
scala> Math.abs(-1)
res3: Int = 1
中缀调用法
语法
对象名 方法名 参数
例如：1 to 10
如果有多个参数，使用括号括起来
示例
使用中缀法Math.abs求绝对值
scala> Math abs -1
res4: Int = 1
操作符即方法
来看一个表达式
1 + 1
大家觉得上面的表达式像不像方法调用？
在scala中，+ - * / %等这些操作符和Java一样，但在scala中，
所有的操作符都是方法
操作符是一个方法名字是符号的方法
花括号调用法
语法
Math.abs{ 
    // 表达式1
    // 表达式2
}
方法只有一个参数，才能使用花括号调用法
示例
使用花括号调用法Math.abs求绝对值
参考代码
scala> Math.abs{-10}
res13: Int = 10
无括号调用法
如果方法没有参数，可以省略方法名后面的括号
示例
定义一个无参数的方法，打印"hello"
使用无括号调用法调用该方法
参考代码
def m3()=println("hello")
m3()

定长数组
定长数组指的是数组的长度是不允许改变的
数组的元素是可以改变的


语法
// 通过指定长度定义数组
1固定长度 val/var 变量名 = new Array[元素类型](数组长度)
// 用元素直接初始化数组
val/var 变量名 = Array(元素1, 元素2, 元素3...)

// 定义包含jave、scala、python三个元素的数组
2不可变 scala> val a = Array("java", "scala", "python")
a: Array[String] = Array(java, scala, python)

变长数组10

0 until n——生成一系列的数字，包含0，不包含n
0 to n ——包含0，也包含n

定义元组
语法
使用括号来定义元组
val/var 元组 = (元素1, 元素2, 元素3....)
使用箭头来定义元组（元组只有两个元素）
val/var 元组 = 元素1->元素2
使用_1、_2、_3....来访问元组中的元素，_1表示访问第一个元素，依次类推


定义
不可变列表就是列表的元素、长度都是不可变的。
语法
使用List(元素1, 元素2, 元素3, ...)来创建一个不可变列表，语法格式：
val/var 变量名 = List(元素1, 元素2, 元素3...)
使用Nil创建一个不可变的空列表
val/var 变量名 = Nil
使用::方法创建一个不可变列表
val/var 变量名 = 元素1 :: 元素2 :: Nil
使用::拼接方式来创建列表，必须在最后添加一个Nil
列表常用操作
以下是列表常用的操作
可变集合都在mutable包中
不可变集合都在immutable包中（默认导入）

定义
使用ListBuffer[元素类型]()创建空的可变列表，语法结构：
val/var 变量名 = ListBuffer[Int]()
使用ListBuffer(元素1, 元素2, 元素3...)创建可变列表，语法结构：
val/var 变量名 = ListBuffer(元素1，元素2，元素3...)
获取元素（使用括号访问(索引值)）
添加元素（+=）
追加一个列表（++=）
更改元素（使用括号获取元素，然后进行赋值）
删除元素（-=）
转换为不可变List（toList）
转换为不可变Array（toArray）

判断列表是否为空（isEmpty）
拼接两个列表（++）
获取列表的首个元素（head）和剩余部分(tail)
反转列表（reverse）
获取前缀（take）、获取后缀（drop）
扁平化（flaten）
拉链（zip）和拉开（unzip）
转换字符串（toString）
生成字符串（mkString）
并集（union）
交集（intersect）
差集（diff）

定义
可变集合不可变集的创建方式一致，只不过需要提前导入一个可变集类。
手动导入：import scala.collection.mutable.Set

不可变Map
var map = Map()  //不管定义的map是否可变,前面用var修饰都能增删元素
val map = Map()  //不可变map不能增删元素,可变map能增删元素



定义
val/var map = Map(键->值, 键->值, 键->值...) // 推荐，可读性更好
val/var map = Map((键, 值), (键, 值), (键, 值), (键, 值)...)
可变Map
定义
定义语法与不可变Map一致。但定义可变Map需要手动导入import scala.collection.mutable.Map
 val map = Map("zhangsan"->30, "lisi"->40) //map: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 30)
map api运用
// 获取zhagnsan的年龄
scala> map("zhangsan") // res10: Int = 30
// 获取所有的学生姓名
scala> map.keys //res13: Iterable[String] = Set(lisi, zhangsan)
// 获取所有的学生年龄
scala> map.values //res14: Iterable[Int] = HashMap(40, 30)
//遍历 打印所有的学生姓名和年龄
scala> for((x,y) <- map) println(s"$x $y") //lisi 40 zhangsan 30
// 获取wangwu的年龄，如果wangwu不存在，则返回-1
scala> map.getOrElse("wangwu", -1) //res17: Int = -1
// 新增一个学生：wangwu, 35
scala> map += "wangwu"->35  //res22: scala.collection.mutable.Map[String,Int] = Map(lisi -> 40, zhangsan -> 30, wangwu -> 35)
// 将lisi从可变映射中移除
scala> map - "lisi" //res23: scala.collection.mutable.Map[String,Int] = Map(zhangsan -> 30)


使用迭代器遍历集合
使用iterator方法可以从集合获取一个迭代器
迭代器的两个基本操作
hasNext——查询容器中是否有下一个元素
next——返回迭代器的下一个元素，如果没有，抛出NoSuchElementException
每一个迭代器都是有状态的
迭代完后保留在最后一个元素的位置
再次使用则抛出NoSuchElementException
可以使用while或者for来逐个返回元素


函数式编程
我们将来使用Spark/Flink的大量业务代码都会使用到函数式编程。下面的这些操作是学习的重点。
遍历（foreach） //foreach(f: (A) ⇒ Unit): Unit  接收一个函数对象 函数的输入参数为集合的元素，返回值为空
当函数参数，只在函数体中出现一次，而且函数体没有嵌套调用时，可以使用下划线来简化函数定义

映射（map） //def map[B](f: (A) ⇒ B): TraversableOnce[B]  //
参数  f: (A) ⇒ B  传入一个函数对象该函数接收一个类型A（要转换的列表元素），
返回值为类型 B 返回值 TraversableOnce[B] B类型的集合

映射扁平化（flatmap） //def flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): TraversableOnce[B] 
泛型[B]:最终要转换的集合元素类型
参数 f: (A) ⇒ GenTraversableOnce[B]: 传入一个函数对象<br />函数的参数是集合的元素<br />函数的返回值是一个集合
返回值 TraversableOnce[B] B类型的集合


过滤（filter）
是否存在（exists）
排序（sorted、sortBy、sortWith） //def sortBy[B](f: (A) ⇒ B): List[A]
分组（groupBy） //def groupBy[K](f: (A) ⇒ K): Map[K, List[A]]
scala> val a = List("张三"->"男", "李四"->"女", "王五"->"男") //a: List[(String, String)] = List((张三,男), (李四,女), (王五,男))

// 按照性别分组
scala> a.groupBy(_._2)  //res0: scala.collection.immutable.Map[String,List[(String, String)]] = Map(男 -> List((张三,男), (王五,男)),女 -> List((李四,女)))

// 将分组后的映射转换为性别/人数元组列表 map.map遍历直接取 key  value  是1,2方式
scala> res0.map(x => x._1 -> x._2.size)  //res3: scala.collection.immutable.Map[String,Int] = Map(男 -> 2, 女 -> 1)

聚合计算（reduce）//def reduce[A1 >: A](op: (A1, A1) ⇒ A1): A1
- reduce和reduceLeft效果一致，表示从左到右计算
- reduceRight表示从右到左计算

泛型[A1 >: A]（下界）A1必须是集合元素类型的子类
参数 op: (A1, A1) ⇒ A1  传入函数对象，用来不断进行聚合操作<br />第一个A1类型参数为：当前聚合后的变量<br />第二个A1类型参数为：当前要进行聚合的元素
返回值 A1 列表最终聚合为一个元素

折叠（fold）fold与reduce很像，但是多了一个指定初始值参数 //def fold[A1 >: A](z: A1)(op: (A1, A1) ⇒ A1): A1
- fold和foldLet效果一致，表示从左往右计算
- foldRight表示从右往左计算



a.foreach((x:Int)=>println(x))
a.foreach(x=>println(x))
a.foreach(println(_))

方法签名

def map[B](f: (A) ⇒ B): TraversableOnce[B]
方法解析
map方法	API	说明
泛型	[B]	指定map方法最终返回的集合泛型
参数	f: (A) ⇒ B	传入一个函数对象
该函数接收一个类型A（要转换的列表元素），返回值为类型B
返回值	TraversableOnce[B]	B类型的集合

map是将列表中的元素转换为一个List
flatten再将整个列表进行扁平化
方法签名
def flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): TraversableOnce[B]
方法解析
flatmap方法	API	说明
泛型	[B]	最终要转换的集合元素类型
参数	f: (A) ⇒ GenTraversableOnce[B]	传入一个函数对象
函数的参数是集合的元素
函数的返回值是一个集合
返回值	TraversableOnce[B]	B类型的集合

方法签名
def filter(p: (A) ⇒ Boolean): TraversableOnce[A]
方法解析
filter方法	API	说明
参数	p: (A) ⇒ Boolean	传入一个函数对象
接收一个集合类型的参数
返回布尔类型，满足条件返回true, 不满足返回false
返回值	TraversableOnce[A]	列表
scala> List(1,2,3,4,5,6,7,8,9).filter(_ % 2 == 0)
方法签名

def sortWith(lt: (A, A) ⇒ Boolean): List[A]
方法解析
sortWith方法	API	说明
参数	lt: (A, A) ⇒ Boolean	传入一个比较大小的函数对象
接收两个集合类型的元素参数
返回两个元素大小，小于返回true，大于返回false
返回值	List[A]	返回排序后的列表
scala> val a = List(2,3,1,6,4,5)
scala> a.sortWith((x,y) => if(x<y)true else false)
res15: List[Int] = List(1, 2, 3, 4, 5, 6)
scala> res15.reverse
res18: List[Int] = List(6, 5, 4, 3, 2, 1)
使用下划线简写上述案例
// 函数参数只在函数中出现一次，可以使用下划线代替
scala> a.sortWith(_ < _).reverse

聚合 | reduce
reduce表示将列表，传入一个函数进行聚合计算



定义
方法签名

def reduce[A1 >: A](op: (A1, A1) ⇒ A1): A1
方法解析

reduce方法	API	说明
泛型	[A1 >: A]	（下界）A1必须是集合元素类型的子类
参数	op: (A1, A1) ⇒ A1	传入函数对象，用来不断进行聚合操作
第一个A1类型参数为：当前聚合后的变量
第二个A1类型参数为：当前要进行聚合的元素
返回值	A1	列表最终聚合为一个元素
reduce和reduceLeft效果一致，表示从左到右计算
reduceRight表示从右到左计算
val a = List(1,2,3,4,5,6,7,8,9,10)
a.reduce((x,y) => x + y)
a.reduce(_ + _)
a.reduceRight(_ + _)


分组 测试
方法签名
折叠 | fold
fold与reduce很像，但是多了一个指定初始值参数
def fold[A1 >: A](z: A1)(op: (A1, A1) ⇒ A1): A1
方法解析
reduce方法	API	说明
泛型	[A1 >: A]	（下界）A1必须是集合元素类型的子类
参数1	z: A1	初始值
参数2	op: (A1, A1) ⇒ A1	传入函数对象，用来不断进行折叠操作
第一个A1类型参数为：当前折叠后的变量
第二个A1类型参数为：当前要进行折叠的元素
返回值	A1	列表最终折叠为一个元素
fold和foldLet效果一致，表示从左往右计算
foldRight表示从右往左计算
a.fold(0)(_ + _)


如果类是空的，没有任何成员，可以省略{}
如果构造器的参数为空，可以省略()




我们还可以使用伴生对象来实现快速创建对象，例如：

// 无需使用new就可以快速来创建对象 val a = Array(1,2,3) val b = Set(1,2,3) 

定义伴生对象
一个class和object具有同样的名字。这个object称为伴生对象，这个class称为伴生类
伴生对象必须要和伴生类一样的名字
伴生对象和伴生类在同一个scala源文件中
伴生对象和伴生类可以互相访问private属性

private[this]访问权限
如果某个成员的权限设置为private[this]，表示只能在当前类中访问。伴生对象也不可以访问

isInstanceOf判断对象是否为指定类的对象
asInstanceOf将对象转换为指定类型

// 使用下划线进行初始化
var name:String = _  //name = null
var age:Int = _ 	//age = 0


示例说明
在scala中，也可以定义抽象的字段。如果一个成员变量是没有初始化，我们就认为它是抽象的。
1. 创建一个Person抽象类，它有一个String抽象字段WHO_AM_I
2. 创建一个Student类，继承自Person类，重写WHO_AM_I字段，初始化为学生
3. 创建一个Policeman类，继承自Person类，重写WHO_AM_I字段，初始化警察
4. 添加main方法，分别创建Student/Policeman的实例，然后分别打印WHO_AM_I


// 定义一个人的抽象类
abstract class Person6 {
  // 没有初始化的val字段就是抽象字段
  val WHO_AM_I:String
}

class Student6 extends Person6 {
  override val WHO_AM_I: String = "学生"
}

class Policeman6 extends Person6 {
  override val WHO_AM_I: String = "警察"
}

object Main6 {
  def main(args: Array[String]): Unit = {
    val p1 = new Student6
    val p2 = new Policeman6

    println(p1.WHO_AM_I)
    println(p2.WHO_AM_I)
  }
}

匿名内部类
val/var 变量名 = new 类/抽象类 {
    // 重写方法
}

对象混入trait
val/var 对象名 = new 类 with 特质  //- 给一个对象添加一些额外的行为,该对象就能调用trait特质方法
  trait Logger {
    def log(msg:String) = println(msg)
  }

  class UserService

  def main(args: Array[String]): Unit = {
    val service = new UserService with Logger
    service.log("混入的方法")
  }


trait调用链
类继承了多个trait后，可以依次调用多个trait中的同一个方法，只要让多个trait中的同一个方法在最后都依次执行super关键字即可。类中调用多个tait中都有这个方法时，首先会从最右边的trait方法开始执行，然后依次往左执行，形成一个调用链条。

trait HandlerTrait {
    def handle(data:String) = println("处理数据...")
}

trait DataValidHanlderTrait extends HandlerTrait {
    override def handle(data:String): Unit = {
        println("验证数据...")
        super.handle(data)
    }
}

trait SignatureValidHandlerTrait extends HandlerTrait {
    override def handle(data: String): Unit = {
        println("校验签名...")
        super.handle(data)
    }
}

class PayService extends DataValidHanlderTrait with SignatureValidHandlerTrait {
    override def handle(data: String): Unit = {
        println("准备支付...")
        super.handle(data)
    }
}

def main(args: Array[String]): Unit = {
    val service = new PayService
    service.handle("支付参数")
}

// 程序运行输出如下：
// 准备支付...
// 检查签名...
// 验证数据...
// 处理数据...

样例类
    case class 样例类名([var/val] 成员变量名1:类型1, 成员变量名2:类型2, 成员变量名3:类型3)
- 如果要实现某个成员变量可以被修改，可以添加var
- 默认为val，可以省略
样例类的方法
当我们定义一个样例类，编译器自动帮助我们实现了以下几个有用的方法：
- apply方法
- toString方法
- equals方法
- hashCode方法
- copy方法

apply方法
apply方法可以让我们快速地使用类名来创建对象。参考以下代码：
    case class CasePerson(name:String, age:Int)
    
    object CaseClassDemo {
      def main(args: Array[String]): Unit = {
        val lisi = CasePerson("李四", 21)
        println(lisi.toString)
      }
    }

toString方法
toString返回样例类名称(成员变量1, 成员变量2, 成员变量3....)，我们可以更方面查看样例类的成员
    case class CasePerson(name:String, age:Int)
    
    object CaseClassDemo {
      def main(args: Array[String]): Unit = {
        val lisi = CasePerson("李四", 21)
        println(lisi.toString)
        // 输出：CasePerson(李四,21)
      }
    }

equals方法
样例类自动实现了equals方法，可以直接使用==比较两个样例类是否相等，即所有的成员变量是否相等
示例
- 创建一个样例类Person，包含姓名、年龄
- 创建名字年龄分别为"李四", 21的两个对象
- 比较它们是否相等
    val lisi1 = CasePerson("李四", 21)
    val lisi2 = CasePerson("李四", 21)
    println(lisi1 == lisi2)
    // 输出：true

hashCode方法
样例类自动实现了hashCode方法，如果所有成员变量的值相同，则hash值相同，只要有一个不一样，则hash值不一样。
- 创建名字年龄分别为"李四", 21的对象
- 再创建一个名字年龄分别为"李四", 22的对象
- 分别打印这两个对象的哈希值
    val lisi1 = CasePerson("李四", 21)
    val lisi2 = CasePerson("李四", 22)
    
    println(lisi1.hashCode())
    println(lisi2.hashCode())

copy方法
样例类实现了copy方法，可以快速创建一个相同的实例对象，可以使用带名参数指定给成员进行重新赋值
示例
- 创建名字年龄分别为"李四", 21的对象
- 通过copy拷贝，名字为"王五"的对象
    val lisi1 = CasePerson("李四", 21)
    val wangwu = lisi1.copy(name="王五")
    println(wangwu)

样例对象	
它主要用在两个地方：
1. 定义枚举
2. 作为没有任何参数的消息传递（后面Akka编程会讲到）
使用case object可以创建样例对象。样例对象是单例的，而且它没有主构造器
语法格式 case object 样例对象名

示例 | 定义枚举
需求说明
- 定义一个性别Sex枚举，它只有两个实例（男性——Male、女性——Female）
- 创建一个Person类，它有两个成员（姓名、性别）
- 创建两个Person对象（"张三"、男性）、（"李四"、"女"）

trait Sex /*定义一个性别特质*/
case object Male extends Sex		// 定义一个样例对象并实现了Sex特质
case object Female extends Sex		

case class Person(name:String, sex:Sex)

object CaseClassDemo {
  def main(args: Array[String]): Unit = {
    val zhangsan = Person("张三", Male)
    println(zhangsan) //Person2(张三,Male)
  }
}

模式匹配(匹配字符串)
scala中有一个非常强大的模式匹配机制，可以应用在很多场景：
- switch语句
- 类型查询
- 使用模式匹配快速获取数据
在Java中，有switch关键字，可以简化if条件判断语句。在scala中，可以使用match表达式替代。
变量 match {
    case "常量1" => 表达式1
    case "常量2" => 表达式2
    case "常量3" => 表达式3
    case _ => 表达式4		// 默认匹配
}

// StdIn.readLine表示从控制台读取一行文本
val name = StdIn.readLine()
val result = name match {
    case "hadoop" => "大数据分布式存储和计算框架"
    case "zookeeper" => "大数据分布式协调服务框架"
    case "spark" => "大数据分布式内存计算框架"
    case _ => "未匹配"
}

匹配模式(匹配类型)
变量 match {
    case 类型1变量名: 类型1 => 表达式1
    case 类型2变量名: 类型2 => 表达式2
    case 类型3变量名: 类型3 => 表达式3
    ...
    case _ => 表达式4
}

val a:Any = "hadoop"
val result = a match {
    case _:String => "String"
    case _:Int => "Int"
    case _:Double => "Double"
}

匹配模式(匹配样例类)
scala可以使用模式匹配来匹配样例类，从而可以快速获取样例类中的成员数据。后续，我们在开发Akka案例时，还会用到。
// 1. 创建两个样例类
case class Person(name:String, age:Int)
case class Order(id:String)

def main(args: Array[String]): Unit = {
    // 2. 创建样例类对象，并赋值为Any类型
    val zhangsan:Any = Person("张三", 20)
    val order1:Any = Order("001")

    // 3. 使用match...case表达式来进行模式匹配
    // 获取样例类中成员变量
    order1 match {
        case Person(name, age) => println(s"姓名：${name} 年龄：${age}")
        case Order(id1) => println(s"ID为：${id1}")
        case _ => println("未匹配")
    }
}

匹配模式(if守卫)	
val a = StdIn.readInt()
a match {
    case _ if a >= 0 && a <= 3 => println("[0-3]")
    case _ if a >= 4 && a <= 8 => println("[3-8]")
    case _ => println("未匹配")
}	

匹配模式(匹配集合)	
Array(1,x,y)   // 以1开头，后续的两个元素不固定
Array(0)	   // 只匹配一个0元素的元素
Array(0, ...)  // 可以任意数量，但是以0开头	

val arr = Array(1, 3, 5)
arr match {
    case Array(1, x, y) => println(x + " " + y)
    case Array(0) => println("only 0")
    case Array(0, _*) => println("0 ...")
    case _ => println("something else")
}

匹配模式(匹配列表)	
List(0)				// 只保存0一个元素的列表
List(0,...)   		// 以0开头的列表，数量不固定
List(x,y)	   		// 只包含两个元素的列表  
	
val list = List(0, 1, 2)
list match {
    case 0 :: Nil => println("只有0的列表")
    case 0 :: tail => println("0开头的列表")
    case x :: y :: Nil => println(s"只有另两个元素${x}, ${y}的列表")
    case _ => println("未匹配")
}

匹配模式(匹配元组)	
示例说明
- 依次修改代码定义以下两个元组
      (1, x, y)		// 以1开头的、一共三个元素的元组
      (x, y, 5)   // 一共有三个元素，最后一个元素为5的元组
- 使用模式匹配上述元素
参考代码
val tuple = (2, 2, 5)
tuple match {
    case (1, x, y) => println(s"三个元素，1开头的元组：1, ${x}, ${y}")
    case (x, y, 5) => println(s"三个元素，5结尾的元组：${x}, ${y}, 5")
    case _ => println("未匹配")
}

匹配模式(声明变量)	
获取数组中的元素
生成包含0-10数字的数组，使用模式匹配分别获取第二个、第三个、第四个元素
val array = (1 to 10).toArray
val Array(_, x, y, z, _*) = array
println(x, y, z)

获取List中的数据
- 生成包含0-10数字的列表，使用模式匹配分别获取第一个、第二个元素
val list = (1 to 10).toList
val x :: y :: tail = list
println(x, y)

匹配模式(提取器(Extractor))	
是不是所有的类都可以进行这样的模式匹配呢？答案是：不可以的。要支持模式匹配，必须要实现一个提取器。

我们之前已经使用过scala中非常强大的模式匹配功能了，通过模式匹配，我们可以快速匹配样例类中的成员变量。例如：
// 1. 创建两个样例类
case class Person(name:String, age:Int)
case class Order(id:String)

def main(args: Array[String]): Unit = {
    // 2. 创建样例类对象，并赋值为Any类型
    val zhangsan:Any = Person("张三", 20)
    val order1:Any = Order("001")

    // 3. 使用match...case表达式来进行模式匹配
    // 获取样例类中成员变量
    order1 match {
        case Person(name, age) => println(s"姓名：${name} 年龄：${age}")
        case Order(id1) => println(s"ID为：${id1}")
        case _ => println("未匹配")
    }
}

定义提取器
语法格式
之前我们学习过了，实现一个类的伴生对象中的apply方法，可以用类名来快速构建一个对象。伴生对象中，还有一个unapply方法。与apply相反，unapply是将该类的对象，拆解为一个个的元素。
要实现一个类的提取器，只需要在该类的伴生对象中实现一个unapply方法即可。
def unapply(stu:Student):Option[(类型1, 类型2, 类型3...)] = {
    if(stu != null) {
        Some((变量1, 变量2, 变量3...))
    }
    else {
        None
    }
}

创建一个Student类，包含姓名年龄两个字段
实现一个类的解构器，并使用match表达式进行模式匹配，提取类中的字段。
class Student(var name:String, var age:Int)

object Student {
    def apply(name:String, age:Int) = {
        new Student(name, age)
    }

    def unapply(student:Student) = {
        val tuple = (student.name, student.age)

        Some(tuple)
    }
}

def main(args: Array[String]): Unit = {
    val zhangsan = Student("张三", 20)

    zhangsan match {
        case Student(name, age) => println(s"${name} => ${age}")
    }
}

Option类型
使用Option类型，可以用来有效避免空引用(null)异常。也就是说，将来我们返回某些数据时，可以返回一个Option类型来替代。
scala中，Option类型来表示可选值。这种类型的数据有两种形式：
- Some(x)：表示实际的值,Some extens Option
- None：表示没有值  None extens Option
使用getOrElse方法，当值为None是可以指定一个默认值

 def dvi(a:Double, b:Double):Option[Double] = {
    if(b != 0) {
      Some(a / b)
    }
    else {
      None
    }
  }

  //方式1
  def main(args: Array[String]): Unit = {
    val result1 = dvi(1.0, 5)
    result1 match {
      case Some(x) => println(x)
      case None => println("除零异常")
    }
  }
  //方式2
  def main(args: Array[String]): Unit = {
    val result = dvi(1, 0).getOrElse(0)
    println(result)
  }
  
偏函数 
- 偏函数被包在花括号内没有match的一组case语句是一个偏函数
- 偏函数是PartialFunction[A, B]的一个实例
  - A代表输入参数类型
  - B代表返回结果类型
//示例1  
val func1: PartialFunction[Int, String] = {
    case 1 => "一"
    case 2 => "二"
    case 3 => "三"
    case _ => "其他"
}
println(func1(2))
//示例2
val list = (1 to 10).toList
val list2 = list.map{
    case x if x >= 1 && x <= 3 => "[1-3]"
    case x if x >= 4 && x <= 8 => "[4-8]"
    case x if x > 8 => "(8-*]"
}
println(list2)


正则表达式Regex类
- scala中提供了Regex类来定义正则表达式
- 要构造一个RegEx对象，直接使用String类的r方法即可
- 建议使用三个双引号来表示正则表达式，不然就得对正则中的反斜杠来进行转义
val regEx = """正则表达式""".r
findAllMatchIn方法
- 使用findAllMatchIn方法可以获取到所有正则匹配到的字符串
val r = """.+@.+\..+""".r
val eml1 = "qq12344@163.com"
val eml2 = "qq12344@.com"
if(r.findAllMatchIn(eml1).size > 0) {
    println(eml1 + "邮箱合法")
}

val regex =""".+@(.+)\..+""".r
val emlList = List("38123845@qq.com", "a1da88123f@gmail.com", "zhansan@163.com", "123afadff.com")

val emlCmpList = emlList.map {
  case x@regex(company) => s"${x} => ${company}"
  case x => x + "=>未知"
}
println(emlCmpList) //List(38123845@qq.com => qq, a1da88123f@gmail.com => gmail, zhansan@163.com => 163, 123afadff.com=>未知)

异常捕获

try {
    // 代码
}
catch {
    case ex:异常类型1 => // 代码
    case ex:异常类型2 => // 代码
}
finally {
    // 代码
}

抛出异常 throw new Exception :scala不需要在方法上声明要抛出的异常，它已经解决了再Java中被认为是设计失败的检查型异常。

泛型 上届[T <: 类型]   下届[T >: 类型]
协变:class Pair[+T] 可以向上转、逆变:class Pair[-T] 可以向下转、非变:class Pair[T]{} 不可以转
class Super
class Sub extends Super

class Temp1[T]
class Temp2[+T]
class Temp3[-T]

def main(args: Array[String]): Unit = {
    val a:Temp1[Sub] = new Temp1[Sub]
    // 编译报错
    // 非变
    //val b:Temp1[Super] = a

    // 协变
    val c: Temp2[Sub] = new Temp2[Sub]
    val d: Temp2[Super] = c

    // 逆变
    val e: Temp3[Super] = new Temp3[Super]
    val f: Temp3[Sub] = e
}

actor
Java并发编程的问题
在Java并发编程中，每个对象都有一个逻辑监视器（monitor），可以用来控制对象的多线程访问。我们添加sychronized关键字来标记，需要进行同步加锁访问。这样，通过加锁的机制来确保同一时间只有一个线程访问共享数据。但这种方式存在资源争夺、以及死锁问题，程序越大问题越麻烦。
1. 定义class或object继承Actor特质
2. 重写act方法
3. 调用Actor的start方法执行Actor
创建两个Actor，一个Actor打印1-10，另一个Actor打印11-20
- 使用class继承Actor创建（如果需要在程序中创建多个相同的Actor）
- 使用object继承Actor创建（如果在程序中只创建一个Actor）

receive方法只接收一次消息，接收完后继续执行act方法
我们可以使用三种方式来发送消息：
  ！   	发送异步消息，没有返回值          
  !?  	发送同步消息，等待返回值          
  !!  	发送异步消息，返回值是Future[Any]
要给actor1发送一个异步字符串消息，使用以下代码： actor1 ! "你好!"

接收消息
Actor中使用receive方法来接收消息，需要给receive方法传入一个偏函数
    {
        case 变量名1:消息类型1 => 业务处理1,
        case 变量名2:消息类型2 => 业务处理2,
        ...
    }
receive方法只接收一次消息，接收完后继续执行act方法



高阶函数
作为值的函数
// 字符串*方法，表示生成指定数量的字符串
val func_num2star = (num:Int) => "*" * num 
print(list.map(func_num2star))
匿名函数
println((1 to 10).map(num => "*" * num))
println((1 to 10).map("*" * _)) // 因为此处num变量只使用了一次，而且只是进行简单的计算，所以可以省略参数列表，使用_替代参数

柯里化：实现对两个数进行计算
def calc_carried(x:Double, y:Double)(func_calc:(Double, Double)=>Double) = {
    func_calc(x, y)
}

//后面的{}为代码块,也是一个函数{(x,y)=>x+y} 函数多行用{}或者直接写方法((x,y)=x+y)
方法当做值传递不用写类型,因为定义函数的时候签名上有类型,
println(calc_carried(10.1, 10.2){
	(x,y) => x + y
})
println(calc_carried(10, 10)(_ + _))
println(calc_carried(10.1, 10.2)(_ * _))
println(calc_carried(100.2, 10)(_ - _))

//下面为柯里化方法用{}
def textFileStream(directory: String): DStream[String] = withNamedScope("text file stream") {
fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString)
}

隐式转换:单个参数
是指以implicit关键字声明的带有单个参数的方法。它是自动被调用的，自动将某种类型转换为另外一种类型。

使用步骤
1. 在object中定义隐式转换方法（使用implicit）
2. 在需要用到隐式转换的地方，引入隐式转换（使用import）
3. 自动调用隐式转化后的方法
class RichFile(val file:File) {
    // 读取文件为字符串
    def read() = {
        Source.fromFile(file).mkString
    }
}

object RichFile {
    // 定义隐式转换方法
    implicit def file2RichFile(file:File) = new RichFile(file)
}

def main(args: Array[String]): Unit = {
    // 加载文件
    val file = new File("./data/1.txt")

    // 导入隐式转换
    import RichFile.file2RichFile

    // file对象具备有read方法
    println(file.read())
}

隐式转换的时机
- 当对象调用类中不存在的方法或者成员时，编译器会自动将对象进行隐式转换
- 当方法中的参数的类型与目标类型不一致时

闭包
闭包其实就是一个函数，只不过这个函数的返回值依赖于声明在函数外部的变量。
可以简单认为，就是可以访问不在当前作用域范围的一个函数。

柯里化就是一个闭包
def add(x:Int)(y:Int) = {
    x + y
  }

def add(x:Int) = {
(y:Int) => x + y
}  

隐式参数
方法可以带有一个标记为implicit的参数列表。这种情况，编译器会查找缺省值，提供给该方法。

1. 在方法后面添加一个参数列表，参数使用implicit修饰
2. 在object中定义implicit修饰的隐式值
3. 调用方法，可以不传入implicit修饰的参数列表，编译器会自动查找缺省值

1. 和隐式转换一样，可以使用import手动导入隐式参数
2. 如果在当前作用域定义了隐式值，会自动进行导入

// 使用implicit定义一个参数
def quote(what:String)(implicit delimiter:(String, String)) = {
    delimiter._1 + what + delimiter._2
}

// 隐式参数
object ImplicitParam {
    implicit val DEFAULT_DELIMITERS = ("<<<", ">>>")
}

def main(args: Array[String]): Unit = {
	// 导入隐式参数
    import ImplicitParam.DEFAULT_DELIMITERS

    println(quote("李雷和韩梅梅")) // <<<李雷和韩梅梅>>>
}


su 用户名
ps aux 查看系统进程
rpm -qa | grep java
ps aux |grep ntpdate

netstat -nap | grep 进程id  //查看占用端口













1 我自己表现怎么样,哪里有不好的地方,以后我应该那些方便改变
2 我这人就干活,不会交流,和刘工交流还多一些
3 来一年了,宋总认为我表现怎么样,评级能不能升
4 感觉自己绩效不好要改正啊,不能一直拉后腿


HDFS ：分布式文件系统
	NameNode：集群当中的主节点，管理元数据(文件的大小，文件的位置，文件的权限)，主要用于管理集群当中的各种数据
	secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理
	DataNode：集群当中的从节点，主要用于存储集群当中的各种数据
MapReduce : 分布式计算系统
Yarn：分布式样集群资源管理



数据计算核心模块：
JobTracker：接收用户的计算请求任务，并分配任务给从节点
TaskTracker：负责执行主节点JobTracker分配的任务


第一种：NameNode与ResourceManager单节点架构模型
文件系统核心模块：
NameNode：集群当中的主节点，主要用于管理集群当中的各种数据
secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理
DataNode：集群当中的从节点，主要用于存储集群当中的各种数据
数据计算核心模块：
ResourceManager：接收用户的计算请求任务，并负责集群的资源分配
NodeManager：负责执行主节点APPmaster分配的任务
第二种：NameNode单节点与ResourceManager高可用架构模型
文件系统核心模块：
NameNode：集群当中的主节点，主要用于管理集群当中的各种数据
secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理
DataNode：集群当中的从节点，主要用于存储集群当中的各种数据
数据计算核心模块：
ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划
分，通过zookeeper实现ResourceManager的高可用
NodeManager：负责执行主节点ResourceManager分配的任务
第三种：NameNode高可用与ResourceManager单节点架构模型
文件系统核心模块：
NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，其中nameNode可以有
两个，形成高可用状态
DataNode：集群当中的从节点，主要用于存储集群当中的各种数据
JournalNode：文件系统元数据信息管理
数据计算核心模块：
ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分
NodeManager：负责执行主节点ResourceManager分配的任务
第四种：NameNode与ResourceManager高可用架构模型
文件系统核心模块：
NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，一般都是使用两个，实
现HA高可用
JournalNode：元数据信息管理进程，一般都是奇数个
DataNode：从节点，用于数据的存储
数据计算核心模块：
ResourceManager：Yarn平台的主节点，主要用于接收各种任务，通过两个，构建成高可用
NodeManager：Yarn平台的从节点，主要用于处理ResourceManager分配的任务

core-site.xml  
	hdfs://node01:8020  hdfs文件链接
	hdfs://node01:50090 SecondaryNameNode地址和端口
	hdfs://node01:50070 nameNode地址端口


cd /export/softwares
tar -zxvf hadoop-2.7.5.tar.gz -C ../servers/

第二步：修改配置文件
修改core-site.xml
第一台机器执行以下命令
cd /export/softwares
tar -zxvf hadoop-2.7.5.tar.gz -C ../servers/
cd /export/servers/hadoop-2.7.5/etc/hadoop

vim core-site.xml
<configuration>
	 <property>
		 <name>fs.default.name</name>
		 <value>hdfs://node01:8020</value>
		 <!-- 是hdfs端口,用于远程连接 -->
	 </property>
	<property>
		 <name>hadoop.tmp.dir</name>  
		 <value>/export/servers/hadoop-2.7.5/hadoopDatas/tempDatas</value>
		 <!-- hadoop.tmp.dir 临时文件 服务端参数，修改需重启 -->
	 </property>
	 <!-- 缓冲区大小，实际工作中根据服务器性能动态调整 -->
	 <property>
		 <name>io.file.buffer.size</name>
		 <value>4096</value>
		 <!-- 在序列文件中使用的读/写缓冲区的大小。单位kb -->
	 </property>
	 <property>
		 <name>fs.trash.interval</name>
		 <value>10080</value>
		 <!-- 开启hdfs的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 -->
	 </property>
</configuration>
 

 <!-- -->
 
修改hdfs-site.xml
第一台机器执行以下命令
cd /export/servers/hadoop-2.7.5/etc/hadoop
vim hdfs-site.xml

<configuration>
 <property>
	 <name>dfs.namenode.secondary.http-address</name>
	 <value>node01:50090</value>
	 <!-- node1:50090 SecondaryNameNode地址和端口-->
 </property>
 <property>
	 <name>dfs.namenode.http-address</name>
	 <value>node01:50070</value>
	  <!--node01:50070 namenode端口地址 -->
 </property>
 <property>
	 <name>dfs.namenode.name.dir</name>
	 <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2</value>
	 <!-- 本地磁盘目录，NN存储fsimage文件的地方。可以是按逗号分隔的目录列表，fsimage文件会存储在全部目录，冗余安全。这里多个目录设定，最好在多个磁盘，另外，如果其中一个磁盘故障，不会导致系统故障，会跳过坏磁盘。由于使用了HA，建议仅设置一个。如果特别在意安全，可以设置2个-->
	 <!-- 定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割 -->
 </property>
 <property>
	 <name>dfs.datanode.data.dir</name>
	 <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2</value>
	 <!-- 本地磁盘目录，HDFS数据应该存储Block的地方。可以是逗号分隔的目录列表（典型的，每个目录在不同的磁盘）。这些目录被轮流使用，一个块存储在这个目录，下一个块存储在下一个目录，依次循环。每个块在同一个机器上仅存储一份。不存在的目录被忽略。必须创建文件夹，否则被视为不存在. -->
 </property>
 
 <property>
	 <name>dfs.namenode.edits.dir</name>
	 <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits</value>
	 <!-- 本地文件，NN存放edits文件的目录。可以是逗号分隔的目录列表。edits文件会存储在每个目录，冗余安全。 -->
 </property>
 
 <property>
	 <name>dfs.namenode.checkpoint.dir</name>
	 <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/snn/name</value>
	 <!--本地文件系统中，DFS SNN应该在哪里存放临时[用于合并|合并后]（to merge）的Image。如果是逗号分隔的目录列表，Image文件存放多份。冗余备份。建议不使用SNN功能，忽略此配置 -->
 </property>
    
 <property>
	 <name>dfs.namenode.checkpoint.edits.dir</name>
	 <value>file:///export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits</value>
	 <!-- 建议不使用SNN功能，忽略此配置 -->
 </property>
 
 <property>
	 <name>dfs.replication</name>
	 <value>3</value>
	 <!-- 数据块副本数。此值可以在创建文件时设定，客户端可以只有设定，也可以在命令行修改。不同文件可以有不同的副本数。默认值用于未指定时。 -->
 </property>
 
 <property>
	 <name>dfs.permissions</name>
	 <value>false</value>
	 <!-- 是否在HDFS中开启权限检查. -->
 </property>
 
 <property>
	 <name>dfs.blocksize</name>
	 <value>134217728</value>
	 <!-- 块大小，字节。可以使用后缀: k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)指定大小 (就像128k, 512m, 1g, 等待) -->
 </property>
</configuration>

修改hadoop-env.sh
第一台机器执行以下命令
cd /export/servers/hadoop-2.7.5/etc/hadoop
vim hadoop-env.sh
export JAVA_HOME=/export/servers/jdk1.8.0_141

修改mapred-site.xml
第一台机器执行以下命令
cd /export/servers/hadoop-2.7.5/etc/hadoop
vim mapred-site.xml
<configuration>
	 <property>
		 <name>mapreduce.job.ubertask.enable</name>
		 <value>true</value>
		 <!-- 是否启用小作业“ubertask”优化，该优化在单个JVM中按顺序运行“足够小”的作业。“小”由以下maxmaps，maxreduces和maxbytes设置定义 -->
	 </property>
	 <property>
		 <name>mapreduce.jobhistory.address</name>
		 <value>node01:10020</value>
		 <!- MapReduce JobHistory服务器IPC主机：端口 --->
	 </property>
	 <property>
		 <name>mapreduce.jobhistory.webapp.address</name>
		 <value>node01:19888</value>
		 <!-- MapReduce JobHistory Server Web UI主机：端口-->
	 </property>
</configuration>

修改yarn-site.xml
第一台机器执行以下命令

cd /export/servers/hadoop-2.7.5/etc/hadoop
vim yarn-site.xml
 <configuration>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>node01</value>
		<!-- 资源调度器对应的hostname -->
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.log-aggregation.retain-seconds</name>
		<value>604800</value>
	</property>
	<property>   
		<name>yarn.nodemanager.resource.memory-mb</name>
		<value>20480</value>
	</property>
	<property> 
		<name>yarn.scheduler.minimum-allocation-mb</name>
		<value>2048</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>2.1</value>
	</property>
	 <property> 
		<name>yarn.resourcemanager.webapp.address</name>
		<value>127.0.0.1:8050</value>
		<!-- 资源调度器对应的端口 可不用-->
	</property>
</configuration>

修改mapred-env.sh
第一台机器执行以下命令
cd /export/servers/hadoop-2.7.5/etc/hadoop
vim mapred-env.sh
export JAVA_HOME=/export/servers/jdk1.8.0_141

修改slaves
修改slaves文件，然后将安装包发送到其他机器，重新启动集群即可
第一台机器执行以下命令
cd /export/servers/hadoop-2.7.5/etc/hadoop
vim slaves
node01
node02
node03
第一台机器执行以下命令
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/tempDatas
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/nn/edits
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/snn/name
mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits

cd /export/servers/
scp -r hadoop-2.7.5 node02:$PWD
scp -r hadoop-2.7.5 node03:$PWD

第三步：配置hadoop的环境变量
三台机器都要进行配置hadoop的环境变量

三台机器执行以下命令
vim /etc/profile
export HADOOP_HOME=/export/servers/hadoop-2.7.5
export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

配置完成之后生效
source /etc/profile

第四步：启动集群
要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个模块。 注意： 首次启动 HDFS 时，必须对
其进行格式化操作。 本质上是一些清理和 准备工作，因为此时的 HDFS 在物理上还是不存在
的。
hdfs namenode -format 或者 hadoop namenode –format
准备启动
第一台机器执行以下命令
cd /export/servers/hadoop-2.7.5/
bin/hdfs namenode -format
sbin/start-dfs.sh
sbin/start-yarn.sh
sbin/mr-jobhistory-daemon.sh start historyserver

三个端口查看界面
http://node01:50070/explorer.html#/ 查看hdfs
http://node01:8088/cluster 查看yarn集群
http://node01:19888/jobhistory 查看历史完成的任务


 HDFS应用场景
2.1 适合的应用场景
存储非常大的文件：这里非常大指的是几百M、G、或者TB级别，需要高吞吐量，对延时
没有要求。
采用流式的数据访问方式: 即一次写入、多次读取，数据集经常从数据源生成或者拷贝一
次，然后在其上做很多分析工作 。
运行于商业硬件上: Hadoop不需要特别贵的机器，可运行于普通廉价机器，可以处节约成
本
需要高容错性
为数据存储提供所需的扩展能力
2.2 不适合的应用场景
1） 低延时的数据访问 对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数
据传输设计的,因此可能牺牲延时
2）大量小文件 文件的元数据保存在NameNode的内存中， 整个文件系统的文件数量会受限
于NameNode的内存大小。 经验而言，一个文件/目录/文件块一般占有150字节的元数据内存
空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。因此十亿级
别的文件数量在现有商用机器上难以支持。
3）多方读写，需要任意的文件修改 HDFS采用追加（append-only）的方式写入数据。不支持
文件任意owset的修改。不支持多个写入器（writer）



nameNode高可用
1 在Hadoop 中，NameNode 所处的位置是非常重要的，整个HDFS文件系统的元数据信息都由NameNode 来管理，NameNode的可用性直接决定了Hadoop 的可用性，一旦NameNode进程不能工作了，就会影响整个集群的正常使用。 
在典型的HA集群中，两台独立的机器被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。Active NameNode负责群集中的所有客户端操作，而Standby充当从服务器。Standby机器保持足够的状态以提供快速故障切换（如果需要）。
2 Hadoop的联邦机制(Federation)
HDFS的运行机制
HDFS是一个 主/从（Mater/Slave）体系结构 ，
HDFS由四部分组成，HDFS Client、NameNod e、DataNode和Secondary NameNode。
nameNode运行机制
namenode对数据的管理采用了三种存储形式：
	内存元数据(NameSystem)  ---metadata
	磁盘元数据镜像文件      ---fsimage
	数据操作日志文件（可通过日志运算出元数据）---edits

DataNode作用
HDFS 文件写入过程
1. Client 发起文件上传请求, 通过 RPC 与 NameNode 建立通讯, NameNode 检查目标文件是否已存在, 父目录是否存在, 返回是否可以上传
2. Client 请求第一个 block 该传输到哪些 DataNode 服务器上
3. NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配, 返回可用的DataNode 的地址如: A, B, C Hadoop 在设计时考虑到数据的安全与高效, 数据文件默认在 HDFS 上存放三份, 存储策略为本地一份, 同机架内其它某一节点上一份, 不同机架的某一节点上一份。
4. Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline）, A 收到请求会继续调用 B, 然后 B 调用 C, 将整个 pipeline 建立完成, 后逐级返回 client
5. Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存）, 以packet 为单位（默认64K）, A 收到一个 packet 就会传给 B, B 传给 C. A 每传一个 packet 会放入一个应答队列等待应答
6. 数据被分割成一个个 packet 数据包在 pipeline 上依次传输, 在 pipeline 反方向上, 逐个发送 ack（命令正确应答）, 最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client
7. 当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个 block 到服务 1

HDFS 文件读取过程
1. Client向NameNode发起RPC请求，来确定请求文件block所在的位置；
2. NameNode会视情况返回文件的部分或者全部block列表，对于每个block，NameNode 都会返回含有该 block 副本的 DataNode 地址； 这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后；
3. Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是DataNode,那么将从本地直接获取数据(短路读取特性)；
4. 底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕；
5. 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表；
6. 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。
7. read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据；
8. 最终读取来所有的 block 会合并成一个完整的最终文件。



 FsImage 和 Edits 详解
edits
edits 存放了客户端最近一段时间的操作日志客户端对 HDFS 进行写文件时会首先被记录在 edits 文件中
edits 修改时元数据也会更新
fsimage
NameNode 中关于元数据的镜像, 一般称为检查点, fsimage 存放了一份比较完整的元数据信息,
因为 fsimage 是 NameNode 的完整的镜像, 如果每次都加载到内存生成树状拓扑结构，这是非常耗内存和CPU, 所以一般开始时对 NameNode 的操作都放在 edits 中
fsimage 内容包含了 NameNode 管理下的所有 DataNode 文件及文件 block 及 block 所在的 DataNode 的元数据信息.
随着 edits 内容增大, 就需要在一定时间点和 fsimage 合并

SecondaryNameNode运行原理
1. SecondaryNameNode 通知 NameNode 切换 editlog
2. SecondaryNameNode 从 NameNode 中获得 fsimage 和 editlog(通过http方式)
3. SecondaryNameNode 将 fsimage 载入内存, 然后开始合并 editlog, 合并之后成为新的
fsimage
4. SecondaryNameNode 将新的 fsimage 发回给 NameNode
5. NameNode 用新的 fsimage 替换旧的 fsimage

完成合并的是 SecondaryNameNode, 会请求 NameNode 停止使用 edits, 暂时将新写操作放入一个新的文件中 edits.new
SecondaryNameNode 从 NameNode 中通过 Http GET 获得 edits, 因为要和 fsimage 合并, 所以也是通过 Http Get 的方式把 fsimage 加载到内存, 然后逐一执行具体对文件系统的操作, 与 fsimage 合并, 生成新的 fsimage, 然后通过 Http POST 的方式把 fsimage 发送给NameNode. NameNode 从 SecondaryNameNode 获得了 fsimage 后会把原有的 fsimage 替换为新的 fsimage, 把 edits.new 变成 edits. 同时会更新 fstime

Hadoop 进入安全模式时需要管理员使用 dfsadmin 的 save namespace 来创建新的检查点
SecondaryNameNode 在合并 edits 和 fsimage 时需要消耗的内存和 NameNode 差不多, 所以一般把 NameNode 和 SecondaryNameNode 放在不同的机器上

java能力提升
1 jvm解决问题能力,jvm不是难点只是一个知识点
2 多线程并发问题解决,性能提升
3 
实体类:所有的操作都是数据结构

解决过什么难题内存泄露
1 内存泄露
2 接口优化性能
3 微服务项目搭建
4 

Tuple	元组
p1arallelism  平行度
shuffle
slot

broadcast  广播变量
withBroadcastSet
ParallelSourceFunction  并行数据
RichParallelSourceFunction 非并行数据
Rich 非
retract  retractStream  收缩流
appendStream  附件流
assignTimestampsAndWatermarks  assign 分配  时间戳  水印



架构设计
Entity 有父子类结构
执行器Service 入参
分结构



1 大数据计算异常,数据丢失怎么处理,怎么备份数据
2 hdfs 数据备份找回
3 怎么动态增加服务,横向扩展服务器,灰度发布,熔断器,限流
4 




https://flink.apache.org/
http://archive.apache.org/dist/flink/flink-1.7.2/

Flink程序需要提交给 Job Client
Job Client将作业提交给 Job Manager
Job Manager负责协调资源分配和作业执行。 资源分配完成后，任务将提交给相应的 Task Manager
Task Manager启动一个线程以开始执行。Task Manager会向Job Manager报告状态更改。例如开始执行，正在进行或已完成。
作业执行完成后，结果将发送回客户端（Job Client）
环境准备:
下载安装包 https://archive.apache.org/dist/flink/flink-1.6.1/flink-1.6.1-bin-hadoop27-scala_2.11.tgz 服务器: node01 (192.168.100.100)

安装步骤：
1.上传压缩包
2.解压
tar -zxvf flink-1.6.1-bin-hadoop27-scala_2.11.tgz -C /export/servers/
启动
cd /export/servers/flink-1.6.1
/bin/start-cluster.sh

使用JPS可以查看到下面两个进程
TaskManagerRunner
StandaloneSessionClusterEntrypoint

slot 在flink里面可以认为是资源组，Flink是通过将任务分成子任务并且将这些子任务分配到slot来并行执行程序。

5.运行测试任务
1bin/flink run /export/servers/flink-1.6.1/examples/batch/WordCount.jar --input /export/servers/zookeeper-3.4.9/zookeeper.out --output /export/servers/flink_data

控制台输出:
Starting execution of program
Program execution finished
Job with JobID bb53dc79d510202e8abd95094791a32b has finished.
Job Runtime: 9539 ms
观察WebUI

2.2. Standalone模式集群安装部署
Standalone集群架构
client客户端提交任务给JobManager
JobManager负责Flink集群计算资源管理，并分发任务给TaskManager执行TaskManager定期向JobManager汇报状态

服务器: node01(Master + Slave)
服务器: node02(Slave)
服务器: node03(Slave)


安装步骤:
1.上传flink压缩包到指定目录
2.解压缩flink到 /export/servers 目录
tar -xvzf flink-1.6.1-bin-hadoop27-scala_2.11.tgz -C /export/servers/

3.使用vi修改 conf/flink-conf.yaml
# jobManager 的IP地址
jobmanager.rpc.address: node01
# JobManager 的端口号
jobmanager.rpc.port: 6123
# JobManager JVM heap 内存大小
jobmanager.heap.size: 1024
# TaskManager JVM heap 内存大小
taskmanager.heap.size: 1024
# 每个 TaskManager 提供的任务 slots 数量大小
taskmanager.numberOfTaskSlots: 2
#是否进行预分配内存，默认不进行预分配，这样在我们不使用flink集群时候不会占用集群资源
taskmanager.memory.preallocate: false
# 程序默认并行计算的个数
parallelism.default: 1
#JobManager的Web界面的端口（默认：8081）
jobmanager.web.port: 8081
#配置每个taskmanager生成的临时文件目录（选配）
taskmanager.tmp.dirs: /export/servers/flink-1.6.1/tmp














slot和parallelism总结
taskmanager.numberOfTaskSlots:2
每一个taskmanager中的分配2个TaskSlot,3个taskmanager一共有6个TaskSlot
parallelism.default:1 运行程序默认的并行度为1，6个TaskSlot只用了1个，有5个空闲

1.slot 是静态的概念，是指 taskmanager 具有的并发执行能力
2.parallelism 是动态的概念，是指程序运行时实际使用的并发能力

4.使用vi修改slaves文件
node01
node02
node03

5.使用vi修改 /etc/profile 系统环境变量配置文件，添加HADOOP_CONF_DIR目录
1export HADOOP_CONF_DIR=/export/servers/hadoop-2.7.5/etc/hadoop

6.分发/etc/profile到其他两个节点
scp -r /etc/profile node02:/etc
scp -r /etc/profile node03:/etc

7.每个节点重新加载环境变量
1source /etc/profile

8.使用scp命令分发flink到其他节点
scp -r /export/servers/flink-1.6.1/ node02:/export/servers/
scp -r /export/servers/flink-1.6.1/ node03:/export/servers/

9.启动Flink集群
./bin/start-cluster.sh

启动/停止flink集群
启动：./bin/start-cluster.sh
停止：./bin/stop-cluster.sh

启动/停止jobmanager 如果集群中的jobmanager进程挂了，执行下面命令启动
bin/jobmanager.sh start
bin/jobmanager.sh stop

启动/停止taskmanager 添加新的taskmanager节点或者重启taskmanager节点
bin/taskmanager.sh start
bin/taskmanager.sh stop

10.启动HDFS集群
cd /export/servers/hadoop-2.7.5/sbin
start-all.sh

11.在HDFS中创建/test/input目录
hdfs dfs -mkdir -p /test/input

12.上传wordcount.txt文件到HDFS /test/input目录
hdfs dfs -put /export/servers/flink-1.6.1/README.txt /test/input

13.并运行测试任务
bin/flink run /export/servers/flink-1.6.1/examples/batch/WordCount.jar --input hdfs://node01:8020/test/input/README.txt --output hdfs://node01:8020/test/output2/result.txt

14.浏览Flink Web UI界面
http://node01:8081

2.3. Standalone高可用HA模式
从上述架构图中，可发现JobManager存在 单点故障 ，一旦JobManager出现意外，整个集群无法工作。所以，为了确保集群的高可用，需要搭建Flink的HA。
HA架构图


环境准备:
服务器: node01(Master + Slave)
服务器: node02(Master + Slave)
服务器: node03(Slave)

安装步骤
1.  在flink-conf.yaml中添加zookeeper配置
#开启HA，使用文件系统作为快照存储
state.backend: filesystem
#启用检查点，可以将快照保存到HDFS
state.backend.fs.checkpointdir: hdfs://node01:8020/flink-checkpoints
#使用zookeeper搭建高可用
high-availability: zookeeper
# 存储JobManager的元数据到HDFS
high-availability.storageDir: hdfs://node01:8020/flink/ha/
high-availability.zookeeper.quorum: node01:2181,node02:2181,node03:2181

2.将配置过的HA的 flink-conf.yaml 分发到另外两个节点
scp -r /export/servers/flink-1.6.1/conf/flink-conf.yaml node02:/export/servers/flink-1.6.1/conf/
scp -r /export/servers/flink-1.6.1/conf/flink-conf.yaml node03:/export/servers/flink-1.6.1/conf/

3.到节点2中修改flink-conf.yaml中的配置，将JobManager设置为自己节点的名称
jobmanager.rpc.address: node02

4.在node01的 masters 配置文件中添加多个节点
node01:8081
node02:8081

5.分发masters配置文件到另外两个节点
scp /export/servers/flink-1.6.1/conf/masters node02:/export/servers/flink-1.6.1/conf/
scp /export/servers/flink-1.6.1/conf/masters node03:/export/servers/flink-1.6.1/conf/

6.启动 zookeeper 集群
7.启动 HDFS 集群
8.启动 flink 集群
9.分别查看两个节点的Flink Web UI
10.kill掉一个节点，查看另外的一个节点的Web UI

注意事项
切记搭建HA，需要将第二个节点的 jobmanager.rpc.address 修改为node02

Yarn集群环境
在一个企业中，为了最大化的利用集群资源，一般都会在一个集群中同时运行多种类型的 Workload。因此 Flink 也支持在 Yarn 上面运行； flink on yarn 的前提是：hdfs、yarn均启动

集群规划
JobManager: node01
WorkManager: node01 node02 node03

步骤
1.修改Hadoop的yarn-site.xml，添加该配置表示内存超过分配值，是否将任务杀掉。默认为true。运行Flink程序，很容易超过分配的内存。
<property>
<name>yarn.nodemanager.vmem-check-enabled</name>
<value>false</value>
</property>

2.分发yarn-site.xml到其它服务器节点
scp yarn-site.xml node02:$PWD
scp yarn-site.xml node03:$PWD

3.启动HDFS、YARN集群
start-all.sh

2.5. yarn-session
Flink运行在YARN上，可以使用 yarn-session 来快速提交作业到YARN集群。我们先来看下Flink On Yarn模式，Flink是如何和Yarn进行交互的。
1.上传jar包和配置文件到HDFS集群上
2.申请资源和请求AppMaster容器
3.Yarn分配资源AppMaster容器，并启动JobManager
JobManager和ApplicationMaster运行在同一个container上。
一旦他们被成功启动，AppMaster就知道JobManager的地址（AM它自己所在的机器）。
它就会为TaskManager生成一个新的Flink配置文件（他们就可以连接到JobManager）。
这个配置文件也被上传到HDFS上。
此外，AppMaster容器也提供了Flink的web服务接口。
YARN所分配的所有端口都是临时端口，这允许用户并行执行多个Flink
4.申请worker资源，启动TaskManager


yarn-session提供两种模式:  会话模式 和 分离模式
2.6. 会话模式
使用Flink中的yarn-session（yarn客户端），会启动两个必要服务 JobManager 和 TaskManager 客户端通过yarn-session提交作业
yarn-session会一直启动，不停地接收客户端提交的作用
有大量的小作业，适合使用这种方式

使用步骤:
1. 在flink目录启动yarn-session
bin/yarn-session.sh -n 2 -tm 800 -s 1 -d
# -n 表示申请2个容器，
# -s 表示每个容器启动多少个slot
# -tm 表示每个TaskManager申请800M内存
# -d 表示以后台程序方式运行
yarn-session.sh脚本可以携带的参数:
   Required
     -n,--container <arg>               分配多少个yarn容器 (=taskmanager的数量)  
   Optional
     -D <arg>                       动态属性
     -d,--detached                   独立运行 （以分离模式运行作业）
     -id,--applicationId <arg>           YARN集群上的任务id，附着到一个后台运行的yarn 
session中
     -j,--jar <arg>                     Path to Flink jar file
     -jm,--jobManagerMemory <arg>     JobManager的内存 [in MB] 
     -m,--jobmanager <host:port>       指定需要连接的jobmanager(主节点)地址  
                                   使用这个参数可以指定一个不同于配置文件中的jobmanager  
     -n,--container <arg>               分配多少个yarn容器 (=taskmanager的数量) 
     -nm,--name <arg>                 在YARN上为一个自定义的应用设置一个名字
     -q,--query                       显示yarn中可用的资源 (内存, cpu核数) 
     -qu,--queue <arg>                 指定YARN队列
     -s,--slots <arg>                   每个TaskManager使用的slots数量
     -st,--streaming                   在流模式下启动Flink
     -tm,--taskManagerMemory <arg>   每个TaskManager的内存 [in MB] 
     -z,--zookeeperNamespace <arg>     针对HA模式在zookeeper上创建NameSpace

2.使用flink提交任务
bin/flink run examples/batch/WordCount.jar

3.如果程序运行完了，可以使用 yarn application -kill application_id 杀掉任务
yarn application -kill application_1554377097889_0002


1.使用flink直接提交任务
bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar
# -m  jobmanager的地址
# -yn 表示TaskManager的个数

2.查看WEB UI
首先是 Checkpoint 机制，这是Flink最重要的一个特性。Flink基于 Chandy-Lamport 算法实现了一个分布式的一致性的快照，从而提供了 一致性的语义 。Chandy-Lamport算法实际上在1985年的时候已经被提出来，但并没有被很广泛的应用，而Flink则把这个算法发扬光大了。Spark最近在实现Continue streaming，Continue streaming的目的是为了降低它处理的延时，其也需要提供这种一致性的语义，最终采用Chandy-Lamport这个算法，说明Chandy-Lamport算法在业界得到了一定的肯定。
提供了一致性的语义之后，Flink为了让用户在编程时能够更轻松、更容易地去 管理状态 ，还提供了一套非常简单明了的State API，包括里面的有ValueState、ListState、MapState，近期添加了BroadcastState，使用State API能够自动享受到这种一致性的语义。
除此之外，Flink还实现了 Watermark 的机制，能够支持基于 事件的时间 的处理，或者说基于系统时间的处理，能够容忍数据的迟到 、容忍乱序 的数据。
另外流计算中一般在对流数据进行操作之前都会先进行开窗，即基于一个什么样的窗口上做这个计算。Flink提供了开箱即用的各种窗口，比如滑动窗口 、滚动窗口 、会话窗口 以及非常灵活的自定义的窗口 。


这里我们选用hive的版本是2.1.1 下载地址为： http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz
下载之后，将我们的安装包上传到第三台机器的/export/soxwares目录下面去



1. having与where不同点
1. where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛
选数据。
2. where后面不能写分组函数，而having后面可以使用分组函数。
3. having只用于group by分组统计语句。

1. order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要
较长的计算时间。
2. sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排
序，并且设置mapred.reduce.tasks>1，则sort by只保证每个reducer的输出有序，不保证全
局有序。
3. distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。
4. cluster by(字段) 除了具有distribute by的功能外，还会对该字段进行排序.
因此，如果distribute 和sort字段是同一个时，此时， cluster by = distribute by + sort by

当distribute by和sort by字段相同时，可以使用cluster by方式。
cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。
以下两种写法等价
select * from score cluster by s_id;
select * from score distribute by s_id sort by s_id;


https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.29-linux-glibc2.12-x86_64.tar.gz




zookeeper 集群通常是用来对用户的分布式应用程序提供协调服务的，为了保证数据的一致性，对 zookeeper 集群进行了这样三种角色划分：leader、follower、observer分别对应着总统、议员和观察者。
总统（leader）：负责进行投票的发起和决议，更新系统状态。
议员（follower）：用于接收客户端请求并向客户端返回结果以及在选举过程中参与投票。
观察者（observer）：也可以接收客户端连接，将写请求转发给leader节点，但是不参与投票过程，只同步leader的状态。通常对查询操作做负载。




core-site.xml  
hdfs://server02:9000  //hdfs端口

hdfs-site.xml
server02:50090 //SecondaryNameNode地址和端口-
server02:50070 //namenode端口地址

mapred-site.xml
server02:10020  //MapReduce JobHistory服务器IPC主机：端口
server02:19888  //MapReduce JobHistory Server Web UI主机：端口

yarn-site.xml
server02  		//资源调度器对应的hostname
127.0.0.1:8050	//资源调度器对应的端口 可不用


118.24.233.44 server01 zk01 hadoop01 storm01
47.105.158.112 server02 zk02 hadoop02 storm02
49.234.89.28 server03 zk03 hadoop03 storm03
	
大数据架构搭建流程
数据分析的两个流程：
实时分析流程：业务数据、消息队列、Storm实时编程、Redis、数据展示（秒级计算）
离线分析流程：不同数据源获取数据、Hadoop集群、数据计算(Hive、Spark、MapReduce)、数据展示（T+1计算

NameNode	HADOOP_NAMENODE_OPTS
DataNode	HADOOP_DATANODE_OPTS
SecondaryNamenode	HADOOP_SECONDARYNAMENODE_OPTS
JobTracker	HADOOP_JOBTRACKER_OPTS
TaskTracker	HADOOP_TASKTRACKER_OPTS


http://47.105.158.112:8888/


scp -r /export/servers/hadoop-2.7.5/etc/hadoop/core-site.xml server03:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/hdfs-site.xml server03:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/mapred-site.xml server03:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/yarn-site.xml server03:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/mapred-env.sh server03:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/slaves server03:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/hadoopDatas server03:/export/servers/hadoop-2.7.5/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/hadoop-env.sh server03:/export/servers/hadoop-2.7.5/etc/hadoop/



mapred-env.sh
yarn-site.xml
hadoop-env.sh
scp -r /export/servers/hadoop-2.7.5/hadoopDatas/temDatas server03:/export/servers/hadoop-2.7.5/hadoopDatas



scp -r /export/servers/hadoop-2.7.5/etc/hadoop/core-site.xml server01:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/hdfs-site.xml server01:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/mapred-site.xml server01:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/yarn-site.xml server01:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/mapred-env.sh server01:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/etc/hadoop/slaves server01:/export/servers/hadoop-2.7.5/etc/hadoop/
scp -r /export/servers/hadoop-2.7.5/hadoopDatas server01:/export/servers/hadoop-2.7.5/

JAVA_HOME

menode.safemode.threshold-pct = 0.9990000128746033

Re-format filesystem in Storage Directory /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas ? (Y or N) Y
Re-format filesystem in Storage Directory /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2 ? (Y or N) Y
Re-format filesystem in Storage Directory /export/servers/hadoop-2.7.5/hadoopDatas/nn/edits ? (Y or N) Y


firewall-cmd --zone=public --add-port=50070/tcp --permanent
firewall-cmd --reload



Hadoop进程50010 datanode，50070 namenode，50090 sec端口被占用



/export/servers/hadoop-2.7.5/hadoopDatas/tempDatas



hadoop是集群是基于zookeeper的其中
除了zookeeper的2888，3888（默认）端口之外，
Hadoop还需要开启的几个端口其中有
zooker存放地址端口:2181
rpc通讯端口:9000（rpc通讯端口只需要namenode与备选namenode开启即可）
http通信端口:50070(同上只需namenode端口开启)
jornalnode端口：8485(数据同步端口需要开放给其它jornalnode服务器使用)
datanode端口:50010
以上端口都是默认的端口可以更具需求改动








