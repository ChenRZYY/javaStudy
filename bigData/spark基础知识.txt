sc.textFile("hdfs://hadoop1.com:9000/workcount.text").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://hadoop1.com:9000/out")

1 .textFile("hdfs://hadoop1.com:9000/workcount.text") 从hdfs中读取数据
2 .flatMaop(_split(" ")) 先Map再压平
3 .map((_,1)) 按照key进行reduce,并将value进行累加
4 .saveAsTextFile("hdfs://hadoop1.com:9000/out") 将结果写入hdfs

启动hadoop
1初始化HDFS bin/hadoop   namenode -format
2启动HDFS sbin/start-dfs.sh
3启动YARN sbin/start-yarn.sh
4停止HDFS sbin/stop-dfs.sh   
5停止YARN sbin/stop-yarn.sh

hadoop端口
http://118.24.233.44:50070

hdfs端口
hdfs://118.24.233.44:9000/workcount.text hdfs://118.24.233.44:9000/out3

启动spark
/usr/spark-2.4.3/sbin/start-all.sh

spark端口
spark://118.24.233.44:7077
sparkUI界面
http://118.24.233.44:8080   界面上分别有master端口,workers端口

启动sparkShell 
/usr/spark-2.4.3/bin/spark-shell master spark://hadoop1.com:7077 executor-memory 512m --total-executor-cores 1

-Dspark.master=spark://118.24.233.44:7077
-Dspark.master=local

spark提交任务jar
/usr/spark-2.4.3/bin/spark-submit --class com.hello.spark.WordCount --master spark://hadoop1.com:7077 --executor-memory 700M --total-executor-cores 1 /root/spark/spark-mvn-1.0-SNAPSHOT.jar hdfs://hadoop1.com:9000/workcount.text hdfs://hadoop1.com:9000/out

/usr/spark-2.4.3/bin/spark-submit --class com.hello.spark.WordCount --master spark://hadoop1.com:7077 --executor-memory 700M --executor-cores 1 /root/spark/spark-mvn-1.0-SNAPSHOT.jar hdfs://hadoop1.com:9000/workcount.text hdfs://hadoop1.com:9000/out
/usr/spark-2.4.3/bin/spark-submit --class com.hello.spark.InferringSchema --master spark://hadoop1.com:7077 --executor-memory 700M --executor-cores 1 /root/spark/spark-mvn-1.0-SNAPSHOT.jar hdfs://hadoop1.com:9000/person.txt hdfs://hadoop1.com:9000/out8

#调试Master，在master节点的spark-env.sh中添加SPARK_MASTER_OPTS变量
export SPARK_MASTER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10000"
#启动Master
sbin/start-master.sh

spark远程调试
#调试Worker，在worker节点的spark-env.sh中添加SPARK_WORKER_OPTS变量
export SPARK_WORKER_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10001"
#启动Worker
sbin/start-slave.sh 1 spark://node-1.itcast.cn:7077

#调试spark-submit + app
bin/spark-submit --class cn.itcast.spark.WordCount --master spark://node-1.itcast.cn:7077 --driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10002" /root/wc.jar hdfs://node-1.itcast.cn:9000/words.txt hdfs://node-1.itcast.cn:9000/out2 

#调试spark-submit + app + executor
bin/spark-submit --class cn.itcast.spark.WordCount --master spark://node-1.itcast.cn:7077 --conf "spark.executor.extraJavaOptions=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10003" --driver-java-options "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10002" /root/wc.jar hdfs://node-1.itcast.cn:9000/words.txt hdfs://node-1.itcast.cn:9000/out2  

/export/servers/spark-2.2.0-bin-hadoop2.7/bin/spark-submit \
--class cn.hp._05_spark.day1_workCount.WordCount \
--master spark://server02:7077 \
--executor-memory 700M --total-executor-cores 1 \
/export/data/hadoop.jar \
hdfs://hadoop1.com:9000/workcount.text hdfs://hadoop1.com:9000/out \


总共有7个端口,NameNode,DataNode,ResourceManager,NodeManager,SecondaryNameNode,Master,Worker
ps -ef|grep java
root      5127     1  0 14:36 ?        00:00:09 /usr/java/jdk1.8.0_161/bin/java -Dproc_namenode org.apache.hadoop.hdfs.server.namenode.NameNode
root      5289     1  0 14:36 ?        00:00:07 /usr/java/jdk1.8.0_161/bin/java -Dproc_datanode org.apache.hadoop.hdfs.server.datanode.DataNode
root      5562     1  0 14:36 ?        00:00:04 /usr/java/jdk1.8.0_161/bin/java -Dproc_secondarynamenode org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
root      8355     1  0 14:40 pts/0    00:00:06 /usr/java/jdk1.8.0_161/bin/java org.apache.spark.deploy.master.Master --host chenzhendong01 --port 7077 --webui-port 8080
root      8476     1  0 14:40 ?        00:00:05 /usr/java/jdk1.8.0_161/bin/java org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://chenzhendong01:7077
root     11593     1  2 14:45 pts/0    00:00:13 /usr/java/jdk1.8.0_161/bin/java org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
root     11739     1  1 14:45 ?        00:00:10 /usr/java/jdk1.8.0_161/bin/java org.apache.hadoop.yarn.server.nodemanager.NodeManager


二 sparkRDD操作
val distData = sc.parallelize(data)
#常用Transformation(即转换，延迟加载)
#通过并行化scala集合创建RDD
val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))
#查看该rdd的分区数量
rdd1.partitions.length

1 val lines = sc.textFile("D:/words.txt")  //读取每行的数据-->返回的类型为MapPartitionsRDD 对象     val linesResult=lines.collect()进行查看
2 val lineLengths = lines.map(s => s.length)  //每行的数据遍历,遍历的结果组成新的RDD -->返回MapPrititionsRDD  val collectResult = lineLengths.collect()
val a = sc.parallelize(1 to 9, 3)  //ParallelCollectionRDD   val rdd=sc.parallelize(Array(1,2,3,4,5,6))
def mapDoubleFunc(a : Int) : (Int,Int) = {
    (a,a*2)
}
val mapResult = a.map(mapDoubleFunc)
println(mapResult.collect().mkString)
rdd.partitions.length  //显示几个分区
val rdd = sc.parallelize(Array(1,2,3,4,5,6), 2x)

val rddBy = rdd.map(x=>x*2).sortBy(x=>x,true) //升序计算
scala> val rddBy = rdd.map(x=>x*2).sortBy(x=>x+"",false).collect
rddBy: Array[Int] = Array(8, 6, 4, 2, 12, 10)		//rdd返回的不一定是String类型的

3 .reduceByKey(_ + _, 1)  //指定几个分区  
4 .filter（function）		//val dateFilter = dateMap.filter(x=>x%3==0) 过滤操作，满足filter内function函数为true的RDD内所有元素组成一个新的数据集。如：filter（a == 1）
val rddBy = rdd.map(x=>x*2).sortBy(x=>x,false).filter(_>=10).collect
rddBy: Array[Int] = Array(12, 10)

5 .flatMap（function）
map是对RDD中元素逐一进行函数操作映射为另外一个RDD，而flatMap操作是将函数应用于RDD之中的每一个元素，将返回的迭代器的所有内容构成新的RDD。而flatMap操作是将函数应用于RDD中每一个元素，将返回的迭代器的所有内容构成RDD。
flatMap与map区别在于map为“映射”，而flatMap“先映射，后扁平化”，map对每一次（func）都产生一个元素，返回一个对象，而flatMap多一步就是将所有对象合并为一个对象。  val dateFlatMap=dateMap.flatMap(x=>(x to 7))
val rdd4 =sc.parallelize(Array("1 2 3","7 8","4 5 6")).flatMap(_.split(" ")).collect
rdd4: Array[String] = Array(1, 2, 3, 7, 8, 4, 5, 6)

6 .mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码
val func = (index: Int, iter: Iterator[(Int)]) => {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func).collect


7 .aggregate###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]
###0 + (0+1+2+3+4   +   0+5+6+7+8+9)
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
rdd1.mapPartitionsWithIndex(func).collect
rdd1.aggregate(0)(_+_, _+_)  //聚合 aggregate 英 [ˈæɡrɪɡət , ˈæɡrɪɡeɪt]

rdd1.aggregate(5)(math.max(_, _), _ + _)  //###5和1比, 得5再和234比得5 --> 5和6789比,得9 --> 5 + (5+9)
val rdd3 = sc.parallelize(List("12","23","345","4567"),2)
rdd3.aggregate("")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y) //24
val rdd4 = sc.parallelize(List("12","23","345",""),2)
rdd4.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y) //10
val rdd5 = sc.parallelize(List("12","23","","345"),2)
rdd5.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y) //11

mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码
val func = (index: Int, iter: Iterator[(Int)]) => {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func).collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
aggregate

def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func1).collect
###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]
###0 + (0+1+2+3+4   +   0+5+6+7+8+9)
rdd1.aggregate(0)(_+_, _+_)
rdd1.aggregate(0)(math.max(_, _), _ + _)
###5和1比, 得5再和234比得5 --> 5和6789比,得9 --> 5 + (5+9)
rdd1.aggregate(5)(math.max(_, _), _ + _)

val rdd2 = sc.parallelize(List("a","b","c","d","e","f"),2)
def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
rdd2.aggregate("")(_ + _, _ + _)
rdd2.aggregate("=")(_ + _, _ + _)

val rdd3 = sc.parallelize(List("12","23","345","4567"),2)
rdd3.aggregate("")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y)

val rdd4 = sc.parallelize(List("12","23","345",""),2)
rdd4.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)

val rdd5 = sc.parallelize(List("12","23","","345"),2)
rdd5.aggregate("")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)


8 aggregateByKey
val pairRDD = sc.parallelize(List( ("cat",2), ("cat", 5), ("mouse", 4),("cat", 12), ("dog", 12), ("mouse", 2)), 2)
def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
  iter.toList.map(x => "[partID:" +  index + ", val: " + x + "]").iterator
}
pairRDD.mapPartitionsWithIndex(func2).collect
pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect

checkpoint
sc.setCheckpointDir("hdfs://node-1.itcast.cn:9000/ck")
val rdd = sc.textFile("hdfs://node-1.itcast.cn:9000/wc").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
rdd.checkpoint
rdd.isCheckpointed
rdd.count
rdd.isCheckpointed
rdd.getCheckpointFile

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
coalesce, repartition
val rdd1 = sc.parallelize(1 to 10, 10)
val rdd2 = rdd1.coalesce(2, false)
rdd2.partitions.length

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
collectAsMap : Map(b -> 2, a -> 1)
val rdd = sc.parallelize(List(("a", 1), ("b", 2)))
rdd.collectAsMap


-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
combineByKey : 和reduceByKey是相同的效果
###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算
###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)-->(hello(1,1),good(1))-->x就相当于hello的第一个1, good中的1
val rdd1 = sc.textFile("hdfs://master:9000/wordcount/input/").flatMap(_.split(" ")).map((_, 1))
val rdd2 = rdd1.combineByKey(x => x, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
rdd1.collect
rdd2.collect

###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加3个10
val rdd3 = rdd1.combineByKey(x => x + 10, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
rdd3.collect


val rdd4 = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)
val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)
val rdd6 = rdd5.zip(rdd4)
val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) => x :+ y, (m: List[String], n: List[String]) => m ++ n)

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
countByKey 

val rdd1 = sc.parallelize(List(("a", 1), ("b", 2), ("b", 2), ("c", 2), ("c", 1)))
rdd1.countByKey
rdd1.countByValue

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
filterByRange

val rdd1 = sc.parallelize(List(("e", 5), ("c", 3), ("d", 4), ("c", 2), ("a", 1)))
val rdd2 = rdd1.filterByRange("b", "d")
rdd2.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
flatMapValues  :  Array((a,1), (a,2), (b,3), (b,4))
val rdd3 = sc.parallelize(List(("a", "1 2"), ("b", "3 4")))
val rdd4 = rdd3.flatMapValues(_.split(" "))
rdd4.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
foldByKey

val rdd1 = sc.parallelize(List("dog", "wolf", "cat", "bear"), 2)
val rdd2 = rdd1.map(x => (x.length, x))
val rdd3 = rdd2.foldByKey("")(_+_)

val rdd = sc.textFile("hdfs://node-1.itcast.cn:9000/wc").flatMap(_.split(" ")).map((_, 1))
rdd.foldByKey(0)(_+_)

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
foreachPartition
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
rdd1.foreachPartition(x => println(x.reduce(_ + _)))

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
keyBy : 以传入的参数做key
val rdd1 = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val rdd2 = rdd1.keyBy(_.length)
rdd2.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
keys values
val rdd1 = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val rdd2 = rdd1.map(x => (x.length, x))
rdd2.keys.collect
rdd2.values.collect

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
mapPartitions

http://118.24.233.44:8080/
http://118.24.233.44:8888/
http://118.24.233.44:50070
https://www.jianshu.com/p/03063227be03

scala> val rdd4 =sc.parallelize(List((List("a b c"),List("w e r"),List("o p i")))).flatMap(_.flatMap(_.split(" "))).collect

#union求并集，注意类型要一致
val rdd6 = sc.parallelize(List(5,6,4,7))
val rdd7 = sc.parallelize(List(1,2,3,4))
val rdd8 = rdd6.union(rdd7)
rdd8.distinct.sortBy(x=>x).collect

#intersection求交集
val rdd9 = rdd6.intersection(rdd7)

#join
val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 2), ("kitty", 3)))
val rdd2 = sc.parallelize(List(("jerry", 9), ("tom", 8), ("shuke", 7)))
val rdd3 = rdd1.join(rdd2)
val rdd3 = rdd1.leftOuterJoin(rdd2)
val rdd3 = rdd1.rightOuterJoin(rdd2)

#groupByKey
val rdd3 = rdd1 union rdd2
rdd3.groupByKey
rdd3.groupByKey.map(x=>(x._1,x._2.sum))

#WordCount, 第二个效率低
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect
sc.textFile("/root/words.txt").flatMap(x=>x.split(" ")).map((_,1)).groupByKey.map(t=>(t._1, t._2.sum)).collect

#cogroup
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)))
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2)))
val rdd3 = rdd1.cogroup(rdd2)
val rdd4 = rdd3.map(t=>(t._1, t._2._1.sum + t._2._2.sum))

#cartesian笛卡尔积
val rdd1 = sc.parallelize(List("tom", "jerry"))
val rdd2 = sc.parallelize(List("tom", "kitty", "shuke"))
val rdd3 = rdd1.cartesian(rdd2)

#spark action
val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)

#collect
rdd1.collect

#reduce
val rdd2 = rdd1.reduce(_+_)

#count
rdd1.count

#top
rdd1.top(2)

#take
rdd1.take(2)

#first(similer to take(1))
rdd1.first

#takeOrdered
rdd1.takeOrdered(3)

三DataFrame DSL风格语法
val lineRDD = sc.textFile("hdfs://hadoop1.com:9000/person.txt").map(_.split(" "))
case class Person(id:Int, name:String, age:Int)
val personRDD = lineRDD.map(x => Person(x(0).toInt, x(1), x(2).toInt))
val personDF = personRDD.toDF
personDF.show

personDF.select(personDF.col("name")).show		//显示name
personDF.select(col("name"), col("age")).show	//显示name
personDF.select("name").show					//显示name

personDF.printSchema		//打印DataFrame的Schema信息

personDF.select(col("id"), col("name"), col("age") + 1).show		//查询所有的name和age，并将age+1
personDF.select(personDF("id"), personDF("name"), personDF("age") + 1).show  //col("id")这是选择这一行的,personDF("name")这是聚合结果

personDF.filter(col("age") >= 18).show		//过滤age大于等于18的

personDF.groupBy("age").count().show()	//按年龄进行分组并统计相同年龄的人数

spark.sqlContext.sql("select * from t_person order by age desc limit 2").show //查询年龄最大的前两名  Spark2.X.X后，想要在Spark-shell中运行这个命令，你需要使用spark.sqlContext.sql()的形式。

sql风格语法
personDF.registerTempTable("t_person") //如果想使用SQL风格的语法，需要将DataFrame注册成表

spark.sqlContext.sql("select * from t_person order by age desc limit 2").show //查询年龄最大的前两名

spark.sqlContext.sql("desc t_person").show //显示表的Schema信息




spark的standalone模式部署
1.安装配置JDK
2.安装配置Spark，修改Spark配置文件(两个配置文件spark-env.sh和slaves)
vim spark-env.sh
#指定JAVA_HOME位置
export JAVA_HOME=/usr/java/jdk1.7.0_45
#指定spark老大Master的IP
export SPARK_MASTER_IP=spark1.itcast.cn
#指定spark老大Master的端口
export SPARK_MASTER_PORT=7077
#指定可用的CPU内核数量(默认:所有可用)
export SPARK_WORKER_CORES=2
#作业可使用的内存容量，默认格式为1000m或者2g(默认:所有RAM去掉给操作系统用的1GB)
export SPARK_WORKER_MEMORY=2g


3.在slaves文件中加入所有Work的地址
node3
node4
node5

4.(可选)配置两个Spark Master实现高可靠(首先要配置zookeeper集群，在spark-env.sh添加SPARK_DAEMON_JAVA_OPTS)
export JAVA_HOME=/usr/java/jdk1.7.0_45
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=192.168.80.10:2181 -Dspark.deploy.zookeeper.dir=/spark"
export SPARK_WORKER_CORES=2
export SPARK_WORKER_MEMORY=2g

export JAVA_HOME=/usr/local/jdk1.7.0_45
export SPARK_MASTER_IP=node-1.itcast.cn
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=2
export SPARK_WORKER_MEMORY=1g

map  作用整条数据
mapPartitions 进行转换  转换函数
mapPartitionsWithIndex   转换函数==>自己可以进行内部过滤

mapValues  只不过map作用于整条数据, mapValue 作用于 Value


sample  把大数据集变小, 尽可能的减少数据集规律的损失,withReplacement: 指定为True的情况下, 可能重复, 如果是Flase, 无重复


filter

intersection 交集
union 并集
subtract 差集

reduce
groupByKey 运算结果的格式: (K, (value1, value2))
reduceByKey 能不能在 Map 端做 Combiner: 1. 能不能减少 IO


	