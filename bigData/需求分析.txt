项目时间：2018.12 - 2020.06
项目名称：中投证券E财富APP离线分析系统
开发环境：Flume+Nginx+Hive+SparkSQL+Sqoop+Azkaban+Oracle+Hadoop
项目描述：项目背景:随着股民用户量的增加以及对互联网证券业务丰富性的需求,项目组开发E财富APP,为更好的定位客户需求,分析客户行为,
因此开发E财富分析系统，行情分析,客户实用度分析,账单分析（包括离线和实时账单分析）。终极意义：改善网站的运营详细了解app指数,提升app功能,才能增加股民的粘性,提高网站流量,提升网站用户体验，让访客更多的沉淀下来变成客户。


pv:	页面加载总次数
uv:	独立访客数
vv:	会话次数
ip: 不重复的ip数

4. 熟悉 Flink 的 DataStream API 与 DataSet API 的使用，Flink on yarn 的任务运行流程，了解 CheckPoint 与SavePoint 的容错机制，waterMark 的延迟触发机制。

项目主要分为三块:大数据就是让数据（日志）说话（分析结果，页面展示，发现问题，解决问题），为决策提供支持
项目时间：2018.12 - 2020.06
项目名称：中投证券E财富APP离线分析系统
开发环境：Flume+Nginx+Hive+SparkSQL+Datax+Azkaban+Oracle+Kudu
第一块Spark离线分析:使用Flume对nginx用户点击日志进行采集导入HDFS,通过Datax将oracle源中数据的历史持仓,历史委托,历史交割,融资融券,历史资产表等导入HDFS进行采集后通过SparkSQL进行指标开发,例如计算历史个股收益,收益率,当月持仓,风格收益,跑赢大盘多少等。
第二块SparkStreaming实时分析：通过pringboot整合Kafka，把用户基本操作买入,卖出,交割流水等采集到Kafka，然后通过SparkStreaming进行实时计算处理，最后存储到HBase。
工作职责：
1、参与项目整体环境架构Hadoop,Spark集群搭建,项目数据仓库的建模分析和表的设计。
2、使用Flume对实时账单的数据进行采集，通过Datax离线同步工具确保跟已有的数据源做到无缝数据同步。
3、使用SparkSQL中的各种算子根据访客的点击行为对离线数据进行pv,uv,平均访问时长,页面使用率。
4、使用SparkSQL对oracle中历史持仓,历史交割对账单,日,月收益率等行为统计分析保存到Kudu中。
5、使用SparkStreaming对kafka中普通交易买入,卖出,对买对卖,批量卖出,数据进行实时数据分析。
6、所有分析出数据保存到Kudu中,从kudu中获取数据展示到页面。


项目一：华为产品中心某项目（CPQ配置价格报价）
微服务系统
开发环境：SpringBoot+SpringCloud+Mybatis+Oracle+Redis+Vue+jdk1.8+Git       ————》服务部署docker，jetty
开发时间/开发周期：2018-3~2018-12
项目描述：CPQ对应的是configure(配置)，price(价格)和Quote(报价)的缩写，是企业销售软件关键组成部分，解决如何迅速创建准确和专业的销售报价单，减少失误和解决效率底下的问题，业界标杆有Oracle,saleSforce。
主要的模块:目录管理(销售配置目录，营销目录)，产品设计，产品上架，服务中台，产品目录管理，产品目录服务，产品规则管理，用户中心，存量产品适配器，界面定制服务，配置计算服务，数据分析服务，例如：与淘宝对比（买一台电脑），淘宝是卖成品的设备，我们是先配置一台电脑（2个kingston金士顿内存条，一个intel4核cpu，一个2G显卡等），然后再生成报价单。微服务有：中台服务，引擎服务，rule服务，config服务。
技术要点：1分库分表，多数据源。2千万级数据量。3 SpringBoot，SpringCloud集成上线，多节点流水线部署。4 多版本数据管理。
负责模块：
1.版本管理：做的新功能为所有对象增加版本管理模块，像qq一样A B C版本一直向上叠加，所有的业务对象都有版本，可以每个版本向下游发布，差量对比数据，复用数据功能。
2.扩展属性服务：简单来说就是为业务对象在表中动态增加字段，先定义一个扩展属性，然后再使用，扩展属性有简单类型，有复杂类型，int，long，string，enum（单选，多选），picture（单图，多图），video（视频），range（区间），text（富文本）。定义好类型以后，该对象就拥有该字段，并拥有该功能。
3.发布功能模块：配置多数据源，把维护库数据同步到发布库的通用表结构jdbc操作，解决发布性能问题。
4.语言国际化：拓展属性国际化语言，根据不同的语言，展示不同的扩展属性元数据。
5.维护界面展示优化：数据库影子表设计，修改产品表以后异步同步数据，并且使用Redis分布式缓存，SpringCache内存缓存优化查询界面。

大数据分析系统
项目时间：2018.12 - 2020.06
项目名称：报价离线分析系统
开发环境：Flume+Nginx+Hive+SparkSQL+Sqoop+Azkaban+Oracle+Hadoop
用户画像模式
1、 参与集群的搭建，数仓分层结构的设计
2、 对多个指标进行开发：每日活跃用户数、周活、月活、每日新增用户数分析.
3、


今日收益

历史开盘收盘价


买入价,收盘价,卖出

收盘价-买入价
当日委托数据保存好 3月收益率= 1年收益率=


提高网站流量，提升网站用户体验，让访客更多的沉淀下来变成会员或客户，通过更少的投入获取最大化的收入。
终极意义：改善网站的运营

第一块Spark离线账单的开发:通过Datax将oracle源中的历史持仓，历史交割，融资融券，历史资产表等进行采集后通过SparkSQL进行指标开发，例如计算历史个股收益，收益率，当月持仓，风格收益，跑赢大盘多少等。
第二块Spark Streaming开发实时账单：通过Datax将MySQL源中的风格股表，现金交割流水表，持仓表等采集到Kafka，然后通过Spark Streaming进行实时计算处理，最后存储到HBase。
第三块数据可视化：通过JavaEE展示在首页“我的账本”中。
工作职责：
1、参与项目整体环境架构Hadoop,Spark集群搭建,项目数据仓库的建模分析和表的设计。
2、使用Flume对实时账单的数据进行采集，通过Datax离线同步工具确保跟已有的数据源做到无缝数据同步。
3、使用SparkSQL中的各种算子对离线部分月账单进行开发工作。
4、结合用户的持仓情况和用户的点击行为统计各项业务指标。


1 离线账单,

在线项目
项目时间：2018.12 - 2020.06
项目名称：恒泰证券后台账单系统
开发环境：Flume+Nginx+Hive+SparkSQL+Sqoop+Azkaban+Oracle+HDFS

自选
	我的自选-->计算那些股票是热股
	持仓 计算那些股票被经常买卖,占比多少
	浏览记录:每天浏览量,浏览多少,时间维度分析
行情 
	综合看盘
	沪深
	板块
	港股
交易
	普通交易:
		买入,卖出,对买对卖,批量卖出,科创板盘后定价买卖,-->使用量
		撤单,持仓,成交,委托,资金流水,对账单,交割单
	银证业务:
		转账,多银行
		查询余额
	打新神器:
		新股,新债,我的可用额度,中签数量
		申购记录,近日可申购新股,查看中签,
	港股通:
		买入,整手卖出,碎股卖出,公司行为,投票申报-->kafka实时统计公司每日实时买入额
		撤单,持仓,成交,委托,对账单,交割单,组和非,交易日
		公司行为:持仓,公司行为委托查询,公司权益
		投票申报:持仓,投票查询,投票权益
	国债逆回购:
		撤单,我的逆回购列表,深市1000起,沪市10万起(1,2,3,4,7,14,28,91,182天期逆回购)
		转入资金,转出资金,当前持有
	基金交易:
		场内基金:买入,卖出,认申购,赎回,场内基金分红设置 撤单,持仓,成交,委托
		分级基金:买入,卖出,认申购,赎回,合并,分拆
		ETF基金 :买入,卖出,认申购,赎回,网上认购
		货币基金:买入,卖出,认申购,赎回
咨询
理财
我的		
		
银证业务: 判断大客户,资金大,
打新神器: 新股新债
港股通: 港股通,
国债逆回购:撤单,我的逆回购列表,深市1000起,沪市10万起-->推荐回报高的股票
基金


信用交易

我的:资产配置,持仓分析,业务办理
行情的计算




flink 扩展视频
简历和面试
1、项目突出解决的行业痛点，工作职责要包括业务点和技术点；
2、突出自己与众不同的地方；
3、项目中解决的问题，帮助团队快速和合作能力；
4、简历不要太啰嗦；
5、找个同伴，一个问一个答，多练，锻炼沟通能力；
6、针对性投简历，去面试，不要怕失败，总结经验；
7、面试时候要了解企业的行业和是做什么的。

数据倾斜
1 经过分析，倾斜的数据主要有以下三种情况:
	1、null（空值）或是一些无意义的信息()之类的,大多是这个原因引起。
	2、无效数据，大量重复的测试数据或是对结果影响不大的有效数据。
	3、有效数据，业务导致的正常数据分布。
2 查找数据倾斜
	通过sparkUI查看task耗时,task数据分配大小
	.sample(false,0.1)样本采样算子,查看哪些key比较多
3 解决数据倾斜
	第1，2种情况，直接对数据进行过滤即可（因为该数据对当前业务不会产生影响）。
	第3种情况则需要进行一些特殊操作，常见的有以下几种做法
		(1) 隔离执行，将异常的key过滤出来单独处理，最后与正常数据的处理结果进行union操作。
		(2) 两阶段join,对key先添加随机值，进行操作后，去掉随机值，再进行一次操作。
		(3) 使用reduceByKey 代替 groupByKey(reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义.)
		(4) 使用map join。
		* 案例
			* 如果使用reduceByKey因为数据倾斜造成运行失败的问题。具体操作流程如下:
				* (1) 将原始的 key 转化为 key + 随机值(例如Random.nextInt)
				* (2) 对数据进行 reduceByKey(func)
				* (3) 将 key + 随机值 转成 key
				* (4) 再对数据进行 reduceByKey(func)
2、spark使用不当造成的数据倾斜
	提高shuffle并行度
		* dataFrame和sparkSql可以设置spark.sql.shuffle.partitions参数控制shuffle的并发度，默认为200。 
		* rdd操作可以设置spark.default.parallelism控制并发度，默认参数由不同的Cluster Manager控制。
		* 局限性: 只是让每个task执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些key的大小非常大，即使一个task单独执行它，也会受到数据倾斜的困扰。
		
	* 使用map join 代替reduce join
		* 在小表不是特别大(取决于你的executor大小)的情况下使用，可以使程序避免shuffle的过程，自然也就没有数据倾斜的困扰了.（详细见http://blog.csdn.net/lsshlsw/article/details/50834858、http://blog.csdn.net/lsshlsw/article/details/48694893）
		* 局限性: 因为是先将小数据发送到每个executor上，所以数据量不能太大。
		
		
		
pv:	页面加载总次数
uv:	独立访客数
vv:	会话次数
ip: 不重复的ip数

13--流量分析常见分类--复合级指标

平均访问频度：一天之内人均会话数
==总的会话次数（session）/总的独立访客数

平均访问深度：一天之内人均浏览页面数
==总的页面浏览数/总的独立访客数

​平均会话时长: 平均每次会话停留的时间
首页跳出率=访问网站且访问一个页面且该页面是首页/总的访问次数


4--分析模型--基础、来源分析模型

pv
uv
ip
vv
对比分析
访问明细
来源分类
搜索词
搜索排行榜
分析模型

15--分析模型--受访、访客分析模型
受访域名,
受访页面,
受访升降榜
地区运营商
终端详情
新老访客

16--分析模型--漏斗分析模型
转化分析
商品搜索
商品浏览
加入购物车
购买

--PageView 浏览次数（PV）:
--Unique Visitor 独立访客（UV）:
--访问次数（VV）：


1 ods表
2 ods-->宽表
3 维度表,
dw_pvs_everyday,
dw_pvs_everymonth,
dw_pvs_everyday,

统计每小时各来访url产生的pv量 dw_pvs_referer_everyhour
统计每小时各来访host的产生的pv数并排序 dw_pvs_refererhost_everyhour
人均浏览页数（平均访问深度） dw_avgpv_user_everyday
平均访问频度 ods_click_stream_visit
平均访问时长 所有会话时长相加
各页面访问统计
每日新访客



