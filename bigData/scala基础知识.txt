1、linux操作基础linux系统简介与安装linux常用命令–文件操作linux常用命令–用户管理与权限linux常用命令–系统管理linux常用命令–免密登陆配置与网络管理linux上常用软件安装linux本地yum源配置及yum软件安装linux防火墙配置linux高级文本处理命令cut、sed、awklinux定时任务crontab2、shell编程shell编程–基本语法shell编程–流程控制shell编程–函数shell编程–综合案例–自动化部署脚本3、内存数据库redisredis和nosql简介redis客户端连接redis的string类型数据结构操作及应用-对象缓存redis的list类型数据结构操作及应用案例-任务调度队列redis的hash及set数据结构操作及应用案例-购物车redis的sortedset数据结构操作及应用案例-排行榜4、布式协调服务zookeeperzookeeper简介及应用场景zookeeper集群安装部署zookeeper的数据节点与命令行操作zookeeper的java客户端基本操作及事件监听zookeeper核心机制及数据节点zookeeper应用案例–分布式共享资源锁zookeeper应用案例–服务器上下线动态感知zookeeper的数据一致性原理及leader选举机制5、java高级特性增强Java多线程基本知识Java同步关键词详解java并发包线程池及在开源软件中的应用Java并发包消息队里及在开源软件中的应用Java JMS技术Java动态代理反射6、轻量级RPC框架开发RPC原理学习Nio原理学习Netty常用API学习轻量级RPC框架需求分析及原理分析轻量级RPC框架开发二、离线计算系统1、hadoop快速入门hadoop背景介绍分布式系统概述离线数据分析流程介绍集群搭建集群使用初步2、HDFS增强HDFS的概念和特性HDFS的shell(命令行客户端)操作HDFS的工作机制NAMENODE的工作机制java的api操作案例1：开发shell采集脚本3、MAPREDUCE详解自定义hadoop的RPC框架Mapreduce编程规范及示例编写Mapreduce程序运行模式及debug方法mapreduce程序运行模式的内在机理mapreduce运算框架的主体工作流程自定义对象的序列化方法MapReduce编程案例4、MAPREDUCE增强Mapreduce排序自定义partitionerMapreduce的combinermapreduce工作机制详解5、MAPREDUCE实战maptask并行度机制-文件切片maptask并行度设置倒排索引共同好友6、federation介绍和hive使用Hadoop的HA机制HA集群的安装部署集群运维测试之Datanode动态上下线集群运维测试之Namenode状态切换管理集群运维测试之数据块的balanceHA下HDFS-API变化hive简介hive架构hive安装部署hvie初使用7、hive增强和flume介绍HQL-DDL基本语法HQL-DML基本语法HIVE的joinHIVE 参数配置HIVE 自定义函数和TransformHIVE 执行HQL的实例分析HIVE最佳实践注意点HIVE优化策略HIVE实战案例Flume介绍Flume的安装部署案例：采集目录到HDFS案例：采集文件到HDFS三、流式计算1、Storm从入门到精通Storm是什么Storm架构分析Storm架构分析Storm编程模型、Tuple源码、并发度分析Storm WordCount案例及常用Api分析Storm集群部署实战Storm+Kafka+Redis业务指标计算Storm源码下载编译Strom集群启动及源码分析Storm任务提交及源码分析Storm数据发送流程分析Storm通信机制分析Storm消息容错机制及源码分析Storm多stream项目分析编写自己的流式任务执行框架2、Storm上下游及架构集成消息队列是什么Kakfa核心组件Kafka集群部署实战及常用命令Kafka配置文件梳理Kakfa JavaApi学习Kafka文件存储机制分析Redis基础及单机环境部署Redis数据结构及典型案例Flume快速入门Flume+Kafka+Storm+Redis整合四、内存计算体系Spark1、scala编程scala编程介绍scala相关软件安装scala基础语法scala方法和函数scala函数式编程特点scala数组和集合scala编程练习（单机版WordCount）scala面向对象scala模式匹配actor编程介绍option和偏函数实战：actor的并发WordCount柯里化隐式转换2、AKKA与RPCAkka并发编程框架实战：RPC编程实战3、Spark快速入门spark介绍spark环境搭建RDD简介RDD的转换和动作实战：RDD综合练习RDD高级算子自定义Partitioner实战：网站访问次数广播变量实战：根据IP计算归属地自定义排序利用JDBC RDD实现数据导入导出WorldCount执行流程详解4、RDD详解RDD依赖关系RDD缓存机制RDD的Checkpoint检查点机制Spark任务执行过程分析RDD的Stage划分5、Spark-Sql应用Spark-SQLSpark结合HiveDataFrame实战：Spark-SQL和DataFrame案例6、SparkStreaming应用实战Spark-Streaming简介Spark-Streaming编程实战：StageFulWordCountFlume结合Spark StreamingKafka结合Spark Streaming窗口函数ELK技术栈介绍ElasticSearch安装和使用Storm架构分析Storm编程模型、Tuple源码、并发度分析Storm WordCount案例及常用Api分析7、Spark核心源码解析Spark源码编译Spark远程debugSpark任务提交行流程源码分析Spark通信流程源码分析SparkContext创建过程源码分析DriverActor和ClientActor通信过程源码分析Worker启动Executor过程源码分析Executor向DriverActor注册过程源码分析Executor向Driver注册过程源码分析DAGScheduler和TaskScheduler源码分析Shuffle过程源码分析Task执行过程源码分析五、机器学习算法1、python及numpy库机器学习简介机器学习与pythonpython语言–快速入门python语言–数据类型详解python语言–流程控制语句python语言–函数使用python语言–模块和包phthon语言–面向对象python机器学习算法库–numpy机器学习必备数学知识–概率论2、常用算法实现knn分类算法–算法原理knn分类算法–代码实现knn分类算法–手写字识别案例lineage回归分类算法–算法原理lineage回归分类算法–算法实现及demo朴素贝叶斯分类算法–算法原理朴素贝叶斯分类算法–算法实现朴素贝叶斯分类算法–垃圾邮件识别应用案例kmeans聚类算法–算法原理kmeans聚类算法–算法实现kmeans聚类算法–地理位置聚类应用决策树分类算法–算法原理决策树分类算法–算法实现在不久的将来，大数据一定会彻底走入我们的生活，有兴趣入行未来前沿产业的朋友，大数据、云计算和物联网的入门知识和资讯信息，让我们一起携手，引领人工智能的未来组合：两个类都是继承同一个父类，那么这两个类就是组合关系。继承：子类使用extends继承父类，那么这两个类就是继承关系。多态：一个父类被多个子类继承，以不同的形式展示。绑定：new一个子类的对象指向父类，但是该对象调用方法的时候，调用的还是子类的方法，和指向没有关系。     3、classOf、isInstanceOf、asInstanceOf区别(1)classOf[T]: 获取类型T的Class对象(2)isInstanceOf[T]: 判断对象是否为T类型的实例。(3)asInstanceOf[T]: 强制类型转换String方法.toBuffer  --> 变成单字符Array.toBuffer -->数组遍历函数定义val f1=(x:Int,y:Int) => x+y方法定义def m1(x:Int,y:Int) :Int=x+y  //可以识别返回类型简写成 def m1(x:Int,y:Int) =x+y 循环遍历for (i <- (0 until 8).reverse) println(i)  //没有返回值 好用的until会生成一个Rangefor (i <- 1 to 10) println(i)		//for循环     for (i <- arr) println(i)			//数组遍历        for (i <- 1 to 3; j <- 1 to 3) print((10 * i + j) + " ")	//双for循环,相当于二维数组	  for (i <- 1 to 3; j <- 1 to 3 if i != j) print((10 * i + j) + " ")  //双for循环 每个生成器都可以带一个条件，注意：if前面没有分号    val v = for (i <- 1 to 10) yield i * 10  //返回要给集合,集合的每个元素就是for循环每次生成的for(i <- (0 until arr.length).reverse) println(arr(i)) //好用的until会生成一个Range //reverse是将前面生成的Range反转	  数组var arr:Array[Int] = new Array[Int](3)val arr1 = new Array[Int](8) //定长数组 简化 val arr2 = Array(1,2,3)val Array(one,two,three)=Array("1","2","3") 匹配性数组,为了定义变量,下面代码可以用one,two等变量定长数组方法val wdArr = arr.map(wd => wd.split(" "))	// map方法这个方法可以遍历数组中元素,通过自己定义的函数可以决定元素如何处理 //取出数组中的元素将元素拆分得到了一个全新的数这个数组中存储的元素时一个数组val flattenArray = wdArr.flatMap			//这时候数组wdArr里的结构为:Array[Array[String]] 一个数组里存着数组所以使用flatMap不定长数组方法val ab = ArrayBuffer[Int]()	//变长数组,缓冲数组val ab1 = arr.reverse.toBuffer //需转换成StringBuffer后输出,不然输出的地址值arr.remove(2)      //删除arr数组下标为2的元素arr.remove(2,3)    //从下标2开始,删除3个元素ab.insert(0, -1)   //在数组某个位置插入元素用insertab.insert(0, -1, 0)//在数组某个位置插入多个元素ab+=8 			  //本数组后面增加一个元素可变参数 * 表示,java用...可变数组,定长数组都能用val sortedArray : Array[Int] = arr.sorted           //升序val sortedArray : Array[Int] = arr.sorted.reverse   //降序val f1 = (x : Int , y: Int) => x < y	//sortWith需要出如一个参数, 参数是一个函数, 这个函数需要有两个参数进行比较, 返回的是一个布尔类型的值val sortwithArray : ArrayBuffer[Int] = arr.sortWith(f1)val sortwithArray = arr.sortWith((x,y) => x < y) 	//2. 简化一下val sortwithArray = arr.sortWith(_ < _) 			//3. 最简化val arr2: Array[Int] = arr.filter(x => x%2 == 0) //遍历过滤val arr3: Array[Int] = arr.map(x => x)	//map把数组中每一个元素都取出来的到一个全新数组arr.foreach(x => println(x))	//foreach数组中的元素取出来并打印 (无返回值) 数组遍历arr.sumarr.maxarr.sortedval arr=Array(("tom",88),("jerry",95))	//将对偶集合转换成映射arr.toMap元组val t=("hadoop",3.14,65535)集合->序列List,集Set,映射MapMapScala中，有两种Map，一个是immutable包下的Map，该Map中的内容不可变；另一个是mutable包下的Map，该Map中的内容可变通常我们在创建一个集合是会用val这个关键字修饰一个变量（相当于java中的final），那么就意味着该变量的引用不可变，该引用中的内容是不是可变，取决于这个引用指向的集合的类型val scores = Map(("tom"->85),("jerry"->99),("kitty"->90))  //mapval scores1 = Map(("tom",85),("jerry",99),("kitty",90)) //第二种方式创建数组scores.getOrElse("suke",0)不可变序列List immutableval lst1 = List(1,2,3) //创建一个不可变的集合//将0插入到lst1的前面生成一个新的List(四种方法都一样)val lst2 = 0 :: lst1   val lst3 = lst1.::(0)   val lst4 = 0 +: lst1  val lst5 = lst1.+:(0)  val lst6 = lst1 :+ 3 //将一个元素添加到lst1的后面产生一个新的集合val lst0 = List(4,5,6)  val lst7 = lst1 ++ lst0     //将2个list合并成一个新的Listval lst8 = lst1 ++: lst0    //将lst1插入到lst0前面生成一个新的集合val lst9 = lst1.:::(lst0)   //将lst0插入到lst1前面生成一个新的集合                             可变序列ListBuffer mutableval lst0 = ListBuffer[Int](1,2,3) //构建一个可变列表，初始有3个元素1,2,3val lst1 = new ListBuffer[Int]    //创建一个空的可变列表lst1 += 4   //向lst1中追加元素，注意：没有生成新的集合lst1.append(5) lst0 ++= lst1  //将lst1中的元素最近到lst0中， 注意：没有生成新的集合val lst2= lst0 ++ lst1   //将lst0和lst1合并成一个新的ListBuffer 注意：生成了一个集合val lst3 = lst0 :+ 5  //将元素追加到lst0的后面生成一个新的集合不可变Setval set1 = new HashSet[Int]()  val set2 = set1 + 4 ////将元素和set1合并生成一个新的set，原有set不变val set3 = set1 ++ Set(5, 6, 7) //两个set相加生成新的set,set中元素不能重复可变Set mutable.HashSetval set1 = new mutable.HashSet[Int]()set1 += 2  //向HashSet中添加元素set1.add(4)  //add等价于+=set1 ++= Set(1,3,5) //set集合相加set1 -= 5   //删除一个元素set1.remove(2)可变mapval map1 = new mutable.HashMap[String, Int]()map1("spark") = 1   //向map中添加数据map1 += (("hadoop", 2))map1.put("storm", 3)map1 -= "spark"   //从map中移除元素map1.remove("hadoop")拉链操作-->如果两个数组的元素个数不一致，拉链操作后生成的数组的长度为较小的那个数组的元素个数arr2.zip(arr) = Array[(String, Int)] = Array((chen,1), (zhen,2), (dong,3))res54.toMap = scala.collection.immutable.Map[String,Int] = Map(chen -> 1, zhen -> 2, dong -> 3)类的定义,构造器,单例对象,拌生对象,apply方法,继承,超类的构造1 重写超类的抽象方法,不需要使用override,重写非抽象方法,必须使用override2 模式匹配(匹配字符串,匹配类型),样例类,偏函数,高阶函数函数val fun1=(x:Int)=>x*2   arr.map(fun1)		  //匿名函数arr.map((x:Int)=>x*3) //匿名函数arr.map(x=>x*5)		//scala可以自动推断参数类型,精简写法arr.map(_*5)		//下划线写法def m(x:Int)= x * 3  val fun2=m _  		//方法转换成函数柯里化:函数编程中，接受多个参数的函数都可以转化为接受单个参数的函数，这个转化过程就叫柯里化，柯里化就是证明了函数只需要一个参数而已。其实我们刚才的学习过程中，已经涉及到了柯里化操作，所以这也印证了，柯里化就是以函数为主体这种思想发展的必然产生的结果。任何多个入参的函数都能转换成单个如此那函数这就叫柯里化,===定义一个函数返回一个函数def m(x:Int)=(y:Int)=>x*yval func=m(3)	//分两次才能调用完	func(5)  =15 //第二次输入参数m(4)(6) = res65: Int = 24 //简易写法al iter= Iterator[(String)])  //new一个集合iter.toList		//转换成listiter.toList.map() //转换成list 对list进行遍历返回一个修改后的Listiter.toList.map().iterator //List转换成迭代器柯里化表达式:下面两个是一样的def mul(x: Int, y: Int) = x * ydef mulCurry(x: Int) = (y: Int) => x * y闭包:就是一个函数把外部的那些不属于自己的对象也包含(闭合)进来。def minusxy(x: Int) = (y: Int) => x - y这就是一个闭包：1) 匿名函数(y: Int) => x -y嵌套在minusxy函数中。2) 匿名函数(y: Int) => x -y使用了该匿名函数之外的变量x3) 函数minusxy返回了引用了局部变量的匿名函数val a = Array("Hello", "World")val b = Array("hello", "world")println(a.corresponds(b)(_.equalsIgnoreCase(_))) //柯里化应用:柯里化是为了让函数更灵活,几个参数都行-->对象的成员变量几个参数都行函数和方法区别//:和= 签名引用的时候不同-->=多用于定义函数,:多用于方法上入参函数类型定义 			val f1 = (x: Int) => x * 2val function = (count: Int) => {count * 2}:Int //最后的Int定义出参类型val f2: (Int) => Int = (x) => x * 2  //匿名函数 f2匿名函数签名引用val f3: () => String = () => "尹正杰"val logBoundDate : (String) => Unit = {//我们在调用log函数时，值传递了第一个参数，第二个参数我们空出来了，并没有传递，而是指定第二个参数的类型。log (date , _: String)}def add2(x:Int)(y:Int) = x + y //柯里化def add3(x:Int) = (y: Int) => x + y  //(y: Int) => x + y 为一个匿名函数, 也就意味着 add3 方法的返回值为一个匿名函数.scala泛型class Person[T] {	def choose[T <: Comparable[T]](firit: T, second: T): T  = {		first	}}隐式转换：我自己的隐式上下文 object MyPredef{	implicit 函数	implicit 值}viewbound要求传入一个隐式转换函数class Chooser[T <% Ordered[T]] {	def bigger(first: T, second: T) : T = {		if(first > second) first else second	}}class Chooser[T] {	def bigger(first: T, second: T)(implicit ord: T => Ordered[T]) : T = {		if(first > second) first else second	}}contextbound要求传入一个隐式转换值class Chooser[T: Ordering] {	def bigger(first: T, second: T) : T = {		val ord = implicitly[Ordering[T]]		if(ord.gt(first, second)) first else second	}}class Chooser[T] {	def bigger(first: T, second: T)(implicit ord : Ordering[T]) : T = {		if(ord.gt(first, second)) first else second	}}[+T][-T]特殊方法val strings: immutable.IndexedSeq[String] = str.map(x => x.hashCode().toString)val iterator: Iterator[String] = strings.iteratorsparkone stack to rules them allRDD Spark-SQL Spark-Streaming Graphx MLlib Master WorkerMaster管理所有的Worker，进而进行资源的调度Worker管理当前计算节点，Worker会启动一个Executor来完成真正的计算spark-env.shslaves对端口的操作(在cmd中输入)①nc -l -p 7777//打开端口，可以向端口中输入文本②nc -l -p 7777 < c:/test.logbin/spark-shell --master spark://node-1.itcast.cn:7077 --executor-memory 2G --total-executor-cores 10http://118.24.233.44:50070/dfshealth.html#tab-datanodehttp://118.24.233.44:8080/ textFile--> HadoopRDD -->MapPartitionsRDD -->RDD  每次生成新的RDD map flatMap reduceByKey sortBy collect    Seqimmutable.IndexedSeq[Int]mvn clean scala:compile compile packagemvn clean scala:compile compile packagemvn clean package -Dmaven.test.skip=true mvn dependency:resolve -Dclassifier=sources 