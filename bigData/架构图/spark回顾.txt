1、序列化
	1、应用场景:文件存储与数据网络传输会使用序列化
	2、spark中有两种序列化:java序列化,kryo序列化
	  项目中一般使用kryo序列化
2、structuredStreaming编程模型:
	1、创建SparkSession对象
	2、通过readStream读取流的数据
	3、数据处理
	4、通过writeStream将数据写入存储介质
	5、启动streaming程序
	6、阻塞主线程
3、原理
	1、无限表
		无限表将流中的数据放入到表中，随着时间流逝，表会不断的延伸
		无限表只是structuredStreaming逻辑上的概念，实际上并不存在
	2、体系结构
		1、Source:数据源
		2、LogicPlan：逻辑执行计划
		3、Sink:数据落地
3、输出模式
	1、OutputModel.Append:输出结果是每个批次的处理结果,在没有聚合操作的时候使用
	2、OutputModel.Complate:输出结果是聚合的全局结果，在有聚合操作的时候用
	3、OutputModel.update:如果有聚合操作，输出结果是全局的更新结果
						  如果没有聚合操作，输出结果与Append结果一样
4、Source	
	1、socket
		.format("scoket").option("host",..).option("port",...).load()
	2、hdfs
		.format("csv").option("sep",..).option("header",...).schema(..).load()
	3、kafka
		.format("kafka").option("topic",..).("kafka.bootstrap.servers",...).load()
5、Sink
	1、HDFS
	2、kafka
	3、foreachWriter
6、Trigger
	Trigger是设置批次时间，如果Trigger时间设置为0,就是默认情况,上一个批次数据处理完成之后，立即开始下一个批次的数据处理
	Trigger是设置批次时间，如果Trigger时间设置大于0,如果上一个批次数据在批次时间内先处理完成，那么会等到批次时间到之后才会开始下一个批次数据处理
8、Source到Sink的流程
	1、通过source的getOffset方法，获取当前处理的offset
	2、将offset写入WAL预写日志
	3、通过source的getBath获取一个批次的数据
	4、拷贝LogicPlan，会对其进行优化，选择最优的一个计划，生成物理执行计划
	5、数据最终通过物理执行计划得到结果，通过Sink的addBath将数据写入存储介质
9、容错语义
	1、at-lest-once:最少一次
	2、at-most-once:最多一次
	3、exactly-once:有且仅有一次
	容错:
		1、WAL预写日志
		2、checkpoint
		3、重放:kafka